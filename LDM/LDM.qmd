---
title: "卒業研究1 第14回報告レポート"
author: "後藤 健一郎"
date: "2026-01-30"
format:
  html:
    toc: true
    toc-depth: 3
  pdf:
    toc: true
    toc-depth: 3
---

## 要旨

本レポートでは Latent Diffusion Modelsの論文に基づき、CelebA-HQ (256px) を対象とした再現実験を報告する。VAE（stabilityai/sd-vae-ft-mse）で画像を潜在空間へ圧縮し、潜在空間上で U-Net による拡散学習を実施した。拡散過程は cosine スケジュール、時間ステップ数 T=1000 を採用し、DDPM の逆拡散で生成した。wandb で学習曲線と生成例を記録し、学習の進行に伴う生成品質の変化を観察した。今後は評価指標の導入と、生成速度・品質の改善が課題である。

## 課題と目的

本課題の目的は、LDM の再現実験を通じて以下を確認することである。

- 論文を読み、既存実装を参考に手元でコードを構築する
- VAE による潜在空間圧縮と拡散学習の一連のパイプラインを構築する
- CelebA-HQ (256px) における学習挙動と生成品質の変化を観察する
- 学習設定（スケジュール・最適化・混合精度）による安定性を検証する

## 用いたデータ

- CelebA-HQ 顔画像データセット（256px）

## 用いた手法

### Latent Diffusion Model (LDM)

LDM は VAE により画像を潜在空間に圧縮し、潜在空間上で拡散モデルを学習する。低次元のlatent spaceで学習を行うことによる学習を行うことにより、計算量とメモリ負荷を低減しつつ、生成品質を維持できる。アーキテクチャは元論文に準拠した。

![本レポートで使用した元論文のアーキテクチャ](./images/LDM_architecture.png)

- 潜在空間学習: VAE によるエンコード・デコード
- 拡散モデル: U-Net + Attention
- ノイズスケジュール: cosine（T=1000）

VAE の downsample factor は自動算出され、本実験では **f=8** となるため、256px 入力から 32×32 の潜在表現を得る。

また、学習開始前にはVAEのlatent spaceからの再構築性を確かめるために比較をしてから学習を始める。
![Fig x. VAEの](./images/reconstruction_check.png)

### DDPM 学習・サンプリング

拡散過程は DDPM の前向き過程でノイズ付加を行い、逆過程で生成を行う。学習は AdamW を用い、CosineAnnealingLR で学習率を調整した。

## 実験設定

表1に主要な実験設定を示す。

| 項目 | 設定値 |
|------|------|
| 計算環境 | Google Colab（A100） |
| データセット | CelebA-HQ |
| 解像度 | 256px |
| VAE | stabilityai/sd-vae-ft-mse（AutoencoderKL） |
| Downsample factor | 8 |
| Latent size | 32×32 |
| Batch | 64 |
| 学習率 | 2e-4 |
| Optimizer | AdamW |
| Scheduler | CosineAnnealingLR（T_max=100000） |
| Total steps | 100000 |
| Grad clip | 1.0 |
| Timesteps | 1000 |
| Beta schedule | cosine |
| Mixed precision | on（GPU 時） |
| Sampler | DDPM |

## 実験結果と考察

### CelebA-HQ

- 学習初期はノイズが強いが、学習の進行に伴い顔の輪郭や髪型などの大域構造が現れる傾向が観察された。
- 生成品質の向上に対して、学習時間と計算資源の負荷が大きい点が課題となった。

![10k](./images/10k.png)

![50k](./images/50k.png)

![100k](./images/100k.png)

![learning-rate](./images/learning_rate.png)

![loss](./images/loss.png)


#### 考察

256px 入力に対して latent size を 32×32 に保てるため、潜在空間での拡散学習は一定の情報量を保持できる。さらに、cosine スケジュールと混合精度の組み合わせにより学習を安定化できた。一方で DDPM の逆拡散は 1000 ステップを要するため生成が重く、今後は高速サンプラー（DDIM など）の導入やステップ削減が有効と考えられる。

## まとめ

- CelebA-HQ (256px) に対して LDM の再現実験を行い、潜在空間学習のパイプラインを構築した。
- 32×32 の潜在表現を用いることで大域構造の再現が可能であった。
- 生成品質と計算コストのバランス改善が今後の課題である。

## やり残した課題・工夫点・苦労した点

### やり残した課題

- FID など定量評価指標の導入
- 生成速度改善のための高速サンプラー（DDIM 等）の実装
- 学習ステップ数・学習率の最適化
    - 学習終盤での学習率が小さすぎる
- 別データセットでの再現性確認

### 工夫した点

- VAE による潜在表現の事前計算とキャッシュにより学習効率を向上させた
- 混合精度学習と勾配クリッピングで学習の安定性を確保した

### 苦労した点

- CelebA-HQ のデータ取得・前処理に時間を要した
- 高解像度設定により学習・生成が重く、試行回数を確保しづらかった

## 参考文献

1. Ho, J., Jain, A., & Abbeel, P. (2020). *Denoising Diffusion Probabilistic Models*. NeurIPS.
2. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). *High-Resolution Image Synthesis with Latent Diffusion Models*. CVPR.
3. Kingma, D. P., & Welling, M. (2014). *Auto-Encoding Variational Bayes*. ICLR.
