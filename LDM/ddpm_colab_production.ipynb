{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e410b0be",
      "metadata": {
        "id": "e410b0be"
      },
      "source": [
        "# LDM (Latent Diffusion Models) - CelebA-HQ / LSUN-Churches Training\n",
        "\n",
        "Google Colab (T4) で動作する\n",
        "\n",
        "**設定**\n",
        "- Dataset: CelebA-HQ または LSUN-Churches\n",
        "- Resolution: 256px\n",
        "- Downsample Factor: f=4 (潜在空間 64×64)\n",
        "- VAE: diffusers の学習済み AutoencoderKL\n",
        "- Scheduler: cosine\n",
        "- wandb でログと生成画像を記録\n",
        "\n",
        "**なぜ f=4 か**\n",
        "- f=8だと 256÷8=32 で潜在変数が小さすぎる\n",
        "- f=4なら 256÷4=64 で顔の表情や建物の柱までクッキリ出る"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f97a434b",
      "metadata": {
        "id": "f97a434b"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6dedb9c",
      "metadata": {
        "id": "e6dedb9c",
        "outputId": "b1c613f5-5bc7-48a3-9291-429ae6863bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jan 12 02:49:19 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# GPU確認\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68106ec",
      "metadata": {
        "id": "e68106ec",
        "outputId": "3788b63f-d195-449b-ad1f-31d3ae9c1357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory: ./outputs\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 出力ディレクトリ\n",
        "import os\n",
        "os.environ['DIFFUSERS_NO_ADVISORY_WARNINGS'] = '1'\n",
        "OUTPUT_DIR = './outputs'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "929c9f92",
      "metadata": {
        "id": "929c9f92",
        "outputId": "efa11c84-e00b-4774-80c0-11ab2ba577cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package installation complete!\n"
          ]
        }
      ],
      "source": [
        "# 必要なパッケージ\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q diffusers transformers accelerate\n",
        "!pip install -q einops tqdm scipy pillow wandb gdown\n",
        "print(\"Package installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145ac604",
      "metadata": {
        "id": "145ac604",
        "outputId": "709e9f93-529b-494f-8fc1-263b657a89fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Config:\n",
            "  Dataset: celebahq\n",
            "  Image size: 128px\n",
            "  Downsample factor: 4\n",
            "  Latent size: 32x32\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Literal, Optional, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', DEVICE)\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    image_size: int = 32\n",
        "    in_channels: int = 4\n",
        "    out_channels: int = 4\n",
        "    model_channels: int = 128\n",
        "    channel_mult: tuple = (1, 2, 3, 4)\n",
        "    num_res_blocks: int = 2\n",
        "    num_heads: int = 8\n",
        "    use_scale_shift_norm: bool = True\n",
        "\n",
        "@dataclass\n",
        "class DiffusionConfig:\n",
        "    timesteps: int = 1000\n",
        "    beta_schedule: Literal['linear', 'cosine', 'quadratic'] = 'cosine'\n",
        "    beta_start: float = 1e-4\n",
        "    beta_end: float = 0.02\n",
        "    s: float = 0.008\n",
        "\n",
        "@dataclass\n",
        "class VAEConfig:\n",
        "    model_id: str = 'stabilityai/sd-vae-ft-mse'\n",
        "    subfolder: Optional[str] = None\n",
        "    downsample_factor: int = 4\n",
        "    latent_channels: int = 4\n",
        "    latent_scaling_factor: float = 0.18215\n",
        "    cache_dir: str = './data/latents'\n",
        "    use_fp16: bool = True\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    dataset: str = 'celebahq'\n",
        "    data_dir: str = './data'\n",
        "    image_size: int = 256\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    batch_size: int = 64\n",
        "    learning_rate: float = 2e-4\n",
        "    total_steps: int = 100_000\n",
        "    grad_clip: float = 1.0\n",
        "    ema_decay: float = 0.9999\n",
        "    save_every: int = 5000\n",
        "    sample_every: int = 5000\n",
        "    log_every: int = 100\n",
        "    num_workers: int = 2\n",
        "    mixed_precision: bool = True\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    model: ModelConfig = field(default_factory=ModelConfig)\n",
        "    diffusion: DiffusionConfig = field(default_factory=DiffusionConfig)\n",
        "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
        "    vae: VAEConfig = field(default_factory=VAEConfig)\n",
        "    data: DataConfig = field(default_factory=DataConfig)\n",
        "    output_dir: str = './outputs'\n",
        "    exp_name: str = 'ldm_celebahq_colab'\n",
        "    seed: int = 42\n",
        "\n",
        "config = Config()\n",
        "config.model.image_size = config.data.image_size // config.vae.downsample_factor\n",
        "config.model.in_channels = config.vae.latent_channels\n",
        "config.model.out_channels = config.vae.latent_channels\n",
        "\n",
        "set_seed(config.seed)\n",
        "print('Config:')\n",
        "print(f'  Dataset: {config.data.dataset}')\n",
        "print(f'  Image size: {config.data.image_size}px')\n",
        "print(f'  Downsample factor: {config.vae.downsample_factor}')\n",
        "print(f'  Latent size: {config.model.image_size}x{config.model.image_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0634597",
      "metadata": {
        "id": "c0634597",
        "outputId": "d938cdc8-22b4-43c4-89d0-6fa4d307403d"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1175724655.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Google Drive を使う場合はここを有効化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "DATA_ROOT = Path('/content/drive/MyDrive/research-experiment/LDM/data')\n",
        "OUTPUT_ROOT = Path('/content/drive/MyDrive/research-experiment/LDM/outputs')\n",
        "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "config.data.data_dir = str(DATA_ROOT)\n",
        "config.output_dir = str(OUTPUT_ROOT)\n",
        "OUTPUT_DIR = config.output_dir\n",
        "print('Data dir:', config.data.data_dir)\n",
        "print('Output dir:', config.output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_ieDxVqU-NNK",
      "metadata": {
        "id": "_ieDxVqU-NNK",
        "outputId": "d9fd9212-30a1-4687-e0af-6873dd3aee94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'config' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-851596971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0mlatent_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_latents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_recompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0mlatent_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-851596971.py\u001b[0m in \u001b[0;36mload_vae\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_fp16\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoencoderKL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "def download_celebahq():\n",
        "    \"\"\"Resolve CelebA-HQ directory. Prefer Drive repo data; no Drive copy.\"\"\"\n",
        "    drive_root = Path('/content/drive/MyDrive')\n",
        "\n",
        "    drive_repo_candidates = [\n",
        "        drive_root / 'research-experiment' / 'LDM' / 'data' / 'celeba_hq_256',\n",
        "        drive_root / 'src' / 'github.com' / 'frinfo702' / 'research-experiment' / 'LDM' / 'data' / 'celeba_hq_256',\n",
        "    ]\n",
        "\n",
        "    drive_candidates = [\n",
        "        drive_root / 'data' / 'LDM' / 'celeba_hq_256',\n",
        "        drive_root / 'data' / 'celeba_hq_256',\n",
        "        drive_root / 'celeba_hq_256',\n",
        "    ]\n",
        "\n",
        "    local_candidates = [\n",
        "        Path(config.data.data_dir) / 'celeba_hq_256',\n",
        "        Path('./data') / 'celeba_hq_256',\n",
        "    ]\n",
        "\n",
        "    def _count_images(p):\n",
        "        return len(list(p.glob('*.jpg'))) + len(list(p.glob('*.png')))\n",
        "\n",
        "    for cand in drive_repo_candidates:\n",
        "        if cand.exists() and _count_images(cand) > 1000:\n",
        "            print(f'CelebA-HQ found in Drive repo: {cand}')\n",
        "            return cand\n",
        "\n",
        "    for cand in drive_candidates:\n",
        "        if cand.exists() and _count_images(cand) > 1000:\n",
        "            print(f'CelebA-HQ found on Drive: {cand}')\n",
        "            return cand\n",
        "\n",
        "    for cand in local_candidates:\n",
        "        if cand.exists() and _count_images(cand) > 1000:\n",
        "            print(f'CelebA-HQ found locally: {cand}')\n",
        "            return cand\n",
        "\n",
        "    local_dir = local_candidates[0]\n",
        "    local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print('Downloading CelebA-HQ dataset...')\n",
        "    url = 'https://drive.google.com/uc?id=1badu11NqxGf6qM3PTTooQDJvQbejgbTv'\n",
        "    output = local_dir / 'celeba_hq.zip'\n",
        "\n",
        "    try:\n",
        "        gdown.download(url, str(output), quiet=False)\n",
        "        print('Extracting...')\n",
        "        with zipfile.ZipFile(output, 'r') as z:\n",
        "            members = z.namelist()\n",
        "            for member in tqdm(members, desc='Extracting', unit='file'):\n",
        "                z.extract(member, local_dir)\n",
        "        output.unlink()  # Remove zip\n",
        "        print('CelebA-HQ download complete!')\n",
        "    except Exception as e:\n",
        "        print(f'Download failed: {e}')\n",
        "        print('Please manually download CelebA-HQ and place images in:', local_dir)\n",
        "        print('Alternative: Use LSUN Churches by changing config.data.dataset to \"lsun_churches\"')\n",
        "\n",
        "    return local_dir\n",
        "\n",
        "\n",
        "def download_lsun_churches():\n",
        "    \"\"\"Download LSUN Churches dataset (subset for training).\"\"\"\n",
        "    data_dir = Path(config.data.data_dir) / 'lsun_churches'\n",
        "    data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    existing_images = list(data_dir.glob('*.jpg')) + list(data_dir.glob('*.webp')) + list(data_dir.glob('*.png'))\n",
        "    if len(existing_images) > 1000:\n",
        "        print(f'LSUN Churches already downloaded: {len(existing_images)} images')\n",
        "        return data_dir\n",
        "\n",
        "    print('Downloading LSUN Churches dataset...')\n",
        "    print('Note: Full LSUN is very large. Using a smaller subset for demo.')\n",
        "    print('Please manually download LSUN Churches and place images in:', data_dir)\n",
        "    print('Or switch to CelebA-HQ by changing config.data.dataset to \"celebahq\"')\n",
        "\n",
        "    return data_dir\n",
        "\n",
        "# Generic ImageFolder dataset\n",
        "class ImageFolderDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, transform=None, extensions=('.jpg', '.jpeg', '.png', '.webp')):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.transform = transform\n",
        "        self.extensions = extensions\n",
        "        self.image_paths = self._find_images()\n",
        "        print(f'Found {len(self.image_paths)} images in {data_dir}')\n",
        "\n",
        "    def _find_images(self):\n",
        "        image_paths = []\n",
        "        for ext in self.extensions:\n",
        "            image_paths.extend(self.data_dir.rglob(f'*{ext}'))\n",
        "            image_paths.extend(self.data_dir.rglob(f'*{ext.upper()}'))\n",
        "        return sorted(image_paths)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, 0  # Dummy label\n",
        "\n",
        "def get_dataloader(dataset_name: str, image_size: int, batch_size: int, num_workers: int, train: bool = True):\n",
        "    \"\"\"Get dataloader for CelebA-HQ or LSUN Churches.\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.RandomHorizontalFlip() if train else transforms.Lambda(lambda x: x),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "    if dataset_name == 'celebahq':\n",
        "        data_dir = download_celebahq()\n",
        "    elif dataset_name == 'lsun_churches':\n",
        "        data_dir = download_lsun_churches()\n",
        "    else:\n",
        "        raise ValueError(f'Unknown dataset: {dataset_name}')\n",
        "\n",
        "    dataset = ImageFolderDataset(str(data_dir), transform=transform)\n",
        "    if len(dataset) == 0:\n",
        "        raise ValueError(f'No images found in {data_dir}')\n",
        "\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=train, num_workers=num_workers, drop_last=train)\n",
        "\n",
        "def load_vae():\n",
        "    dtype = torch.float16 if (config.vae.use_fp16 and DEVICE.type == 'cuda') else torch.float32\n",
        "    vae = AutoencoderKL.from_pretrained(config.vae.model_id, subfolder=config.vae.subfolder)\n",
        "    vae = vae.to(DEVICE, dtype=dtype).eval()\n",
        "    for p in vae.parameters():\n",
        "        p.requires_grad = False\n",
        "    return vae\n",
        "\n",
        "@torch.no_grad()\n",
        "def prepare_latents(vae, force_recompute: bool = False):\n",
        "    cache_dir = Path(config.vae.cache_dir)\n",
        "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "    safe_id = config.vae.model_id.replace('/', '_')\n",
        "    cache_path = cache_dir / f\"{config.data.dataset}_{config.data.image_size}_f{config.vae.downsample_factor}_{safe_id}_latents.pt\"\n",
        "\n",
        "    if cache_path.exists() and not force_recompute:\n",
        "        payload = torch.load(cache_path, map_location='cpu')\n",
        "        dataset = TensorDataset(payload['latents'], payload['labels'])\n",
        "        print(f'Loaded cached latents: {cache_path}')\n",
        "        print(f'  Shape: {payload[\"latents\"].shape}')\n",
        "        return dataset\n",
        "\n",
        "    loader = get_dataloader(config.data.dataset, config.data.image_size, config.training.batch_size, config.training.num_workers, train=True)\n",
        "\n",
        "    print(f'Encoding latents for {config.data.dataset} at {config.data.image_size}px...')\n",
        "    print(f'  VAE: {config.vae.model_id}')\n",
        "    print(f'  Downsample factor: {config.vae.downsample_factor}')\n",
        "    print(f'  Expected latent size: {config.model.image_size}x{config.model.image_size}')\n",
        "\n",
        "    latents_list, labels_list = [], []\n",
        "    for images, labels in tqdm(loader, desc='Encoding latents'):\n",
        "        images = images.to(DEVICE)\n",
        "        if config.vae.use_fp16 and DEVICE.type == 'cuda':\n",
        "            images = images.half()\n",
        "        scale = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
        "        dist = vae.encode(images).latent_dist\n",
        "        latents = dist.sample() * scale\n",
        "        latents_list.append(latents.cpu())\n",
        "        labels_list.append(labels.cpu())\n",
        "\n",
        "    latents = torch.cat(latents_list, dim=0)\n",
        "    labels = torch.cat(labels_list, dim=0)\n",
        "    torch.save({'latents': latents, 'labels': labels}, cache_path)\n",
        "    print(f'Saved latents to: {cache_path}')\n",
        "    print(f'  Shape: {latents.shape}')\n",
        "    return TensorDataset(latents, labels)\n",
        "\n",
        "vae = load_vae()\n",
        "# Sync config with actual VAE settings (fix downsample mismatch + scaling)\n",
        "config.vae.latent_scaling_factor = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
        "config.vae.downsample_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n",
        "config.model.image_size = config.data.image_size // config.vae.downsample_factor\n",
        "config.model.in_channels = vae.config.latent_channels\n",
        "config.model.out_channels = vae.config.latent_channels\n",
        "\n",
        "print(f'VAE scaling_factor: {config.vae.latent_scaling_factor}')\n",
        "print(f'VAE downsample_factor: {config.vae.downsample_factor}')\n",
        "print(f'Latent size: {config.model.image_size}x{config.model.image_size}')\n",
        "latent_dataset = prepare_latents(vae, force_recompute=False)\n",
        "latent_loader = DataLoader(latent_dataset, batch_size=config.training.batch_size, shuffle=True, num_workers=config.training.num_workers, drop_last=True)\n",
        "print(f'Latent batches: {len(latent_loader)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vae_check_cell",
      "metadata": {
        "id": "vae_check_cell"
      },
      "outputs": [],
      "source": [
        "# VAE Reconstruction Check\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def check_vae_reconstruction(vae, batch_size=2):\n",
        "    # Create a temporary loader for raw images\n",
        "    check_loader = get_dataloader(\n",
        "        config.data.dataset, \n",
        "        config.data.image_size, \n",
        "        batch_size, \n",
        "        num_workers=1, \n",
        "        train=True\n",
        "    )\n",
        "\n",
        "    images, _ = next(iter(check_loader))\n",
        "    images = images.to(DEVICE)\n",
        "    if config.vae.use_fp16 and DEVICE.type == 'cuda':\n",
        "        images = images.half()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        dist = vae.encode(images).latent_dist\n",
        "        latents = dist.sample()\n",
        "\n",
        "        recons = vae.decode(latents).sample\n",
        "\n",
        "    images = (images * 0.5 + 0.5).clamp(0, 1).cpu()\n",
        "    recons = (recons * 0.5 + 0.5).clamp(0, 1).cpu()\n",
        "\n",
        "    fig, axes = plt.subplots(batch_size, 2, figsize=(10, 4 * batch_size))\n",
        "    if batch_size == 1: axes = [axes]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        ax_orig = axes[i][0] if batch_size > 1 else axes[0]\n",
        "        ax_recon = axes[i][1] if batch_size > 1 else axes[1]\n",
        "\n",
        "        ax_orig.imshow(images[i].permute(1, 2, 0))\n",
        "        ax_orig.set_title(\"Original\")\n",
        "        ax_orig.axis('off')\n",
        "\n",
        "        ax_recon.imshow(recons[i].permute(1, 2, 0))\n",
        "        ax_recon.set_title(\"Reconstructed\")\n",
        "        ax_recon.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Checking VAE reconstruction quality...\")\n",
        "check_vae_reconstruction(vae, batch_size=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "298a2ba6",
      "metadata": {
        "id": "298a2ba6"
      },
      "source": [
        "## 2. Imports & Config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f314ef7",
      "metadata": {
        "id": "1f314ef7"
      },
      "source": [
        "## 3. Dataset Download & Dataloader\n",
        "\n",
        "CelebA-HQ または LSUN-Churches をダウンロードします。\n",
        "どちらを使うかは上のConfigで `config.data.dataset` を変更してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b33d09bc",
      "metadata": {
        "id": "b33d09bc"
      },
      "source": [
        "## 4. UNet (Latent Space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff95ffb5",
      "metadata": {
        "id": "ff95ffb5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "    def forward(self, time: torch.Tensor) -> torch.Tensor:\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return super().forward(x.float()).type(x.dtype)\n",
        "\n",
        "def normalization(channels: int, num_groups: int = 32) -> nn.Module:\n",
        "    return GroupNorm32(min(num_groups, channels), channels)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, dropout=0.1, use_scale_shift_norm=True):\n",
        "        super().__init__()\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "        self.norm1 = normalization(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.norm2 = normalization(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        time_out_dim = out_channels * 2 if use_scale_shift_norm else out_channels\n",
        "        self.time_mlp = nn.Sequential(Swish(), nn.Linear(time_emb_dim, time_out_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.skip = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
        "    def forward(self, x, time_emb):\n",
        "        h = self.norm1(x)\n",
        "        h = F.silu(h)\n",
        "        h = self.conv1(h)\n",
        "        time_emb = self.time_mlp(time_emb)\n",
        "        time_emb = time_emb[:, :, None, None]\n",
        "        if self.use_scale_shift_norm:\n",
        "            scale, shift = time_emb.chunk(2, dim=1)\n",
        "            h = self.norm2(h) * (1 + scale) + shift\n",
        "        else:\n",
        "            h = h + time_emb\n",
        "            h = self.norm2(h)\n",
        "        h = F.silu(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "        return h + self.skip(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels: int, num_heads: int = 4):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = channels // num_heads\n",
        "        self.norm = normalization(channels)\n",
        "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
        "        self.proj = nn.Conv2d(channels, channels, 1)\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x_norm = self.norm(x)\n",
        "        qkv = self.qkv(x_norm)\n",
        "        qkv = qkv.reshape(b, 3, self.num_heads, self.head_dim, h * w)\n",
        "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
        "        q = q.permute(0, 1, 3, 2)\n",
        "        k = k.permute(0, 1, 3, 2)\n",
        "        v = v.permute(0, 1, 3, 2)\n",
        "        attn = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.permute(0, 1, 3, 2).reshape(b, c, h, w)\n",
        "        out = self.proj(out)\n",
        "        return x + out\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, stride=2, padding=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.image_size = cfg.image_size\n",
        "        time_emb_dim = cfg.model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(cfg.model_channels),\n",
        "            nn.Linear(cfg.model_channels, time_emb_dim),\n",
        "            Swish(),\n",
        "            nn.Linear(time_emb_dim, time_emb_dim),\n",
        "        )\n",
        "        self.init_conv = nn.Conv2d(cfg.in_channels, cfg.model_channels, 3, padding=1)\n",
        "\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "\n",
        "        channels = [cfg.model_channels]\n",
        "        ch = cfg.model_channels\n",
        "        resolution = cfg.image_size\n",
        "\n",
        "        for level, mult in enumerate(cfg.channel_mult):\n",
        "            out_ch = cfg.model_channels * mult\n",
        "            for _ in range(cfg.num_res_blocks):\n",
        "                layers = [ResidualBlock(ch, out_ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm)]\n",
        "                ch = out_ch\n",
        "                if resolution in cfg.attention_resolutions:\n",
        "                    layers.append(AttentionBlock(ch, cfg.num_heads))\n",
        "                self.downs.append(nn.ModuleList(layers))\n",
        "                channels.append(ch)\n",
        "            if level != len(cfg.channel_mult) - 1:\n",
        "                self.downs.append(nn.ModuleList([Downsample(ch)]))\n",
        "                channels.append(ch)\n",
        "                resolution //= 2\n",
        "\n",
        "        self.mid = nn.ModuleList([\n",
        "            ResidualBlock(ch, ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm),\n",
        "            AttentionBlock(ch, cfg.num_heads),\n",
        "            ResidualBlock(ch, ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm),\n",
        "        ])\n",
        "\n",
        "        for level, mult in enumerate(reversed(cfg.channel_mult)):\n",
        "            out_ch = cfg.model_channels * mult\n",
        "            for i in range(cfg.num_res_blocks + 1):\n",
        "                skip_ch = channels.pop()\n",
        "                layers = [ResidualBlock(ch + skip_ch, out_ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm)]\n",
        "                ch = out_ch\n",
        "                if resolution in cfg.attention_resolutions:\n",
        "                    layers.append(AttentionBlock(ch, cfg.num_heads))\n",
        "                if level != len(cfg.channel_mult) - 1 and i == cfg.num_res_blocks:\n",
        "                    layers.append(Upsample(ch))\n",
        "                    resolution *= 2\n",
        "                self.ups.append(nn.ModuleList(layers))\n",
        "\n",
        "        self.final_norm = normalization(ch)\n",
        "        self.final_conv = nn.Conv2d(ch, cfg.out_channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t_emb = self.time_embed(t)\n",
        "        h = self.init_conv(x)\n",
        "        hs = [h]\n",
        "        for layers in self.downs:\n",
        "            for layer in layers:\n",
        "                if isinstance(layer, ResidualBlock):\n",
        "                    h = layer(h, t_emb)\n",
        "                else:\n",
        "                    h = layer(h)\n",
        "            hs.append(h)\n",
        "        for layer in self.mid:\n",
        "            if isinstance(layer, ResidualBlock):\n",
        "                h = layer(h, t_emb)\n",
        "            else:\n",
        "                h = layer(h)\n",
        "        for layers in self.ups:\n",
        "            h = torch.cat([h, hs.pop()], dim=1)\n",
        "            for layer in layers:\n",
        "                if isinstance(layer, ResidualBlock):\n",
        "                    h = layer(h, t_emb)\n",
        "                else:\n",
        "                    h = layer(h)\n",
        "        h = self.final_norm(h)\n",
        "        h = F.silu(h)\n",
        "        return self.final_conv(h)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e753c987",
      "metadata": {
        "id": "e753c987"
      },
      "source": [
        "## 5. Diffusion + EMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cb10ea3",
      "metadata": {
        "id": "6cb10ea3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cosine_beta_schedule(timesteps: int, s: float = 0.008) -> torch.Tensor:\n",
        "    steps = timesteps + 1\n",
        "    t = torch.linspace(0, timesteps, steps) / timesteps\n",
        "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clamp(betas, 0.0001, 0.9999)\n",
        "\n",
        "class GaussianDiffusion(nn.Module):\n",
        "    def __init__(self, model, cfg: DiffusionConfig):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.timesteps = cfg.timesteps\n",
        "        betas = cosine_beta_schedule(cfg.timesteps, cfg.s)\n",
        "        alphas = 1.0 - betas\n",
        "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "        self.register_buffer('betas', betas)\n",
        "        self.register_buffer('alphas', alphas)\n",
        "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
        "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
        "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
        "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - alphas_cumprod))\n",
        "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod))\n",
        "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod - 1))\n",
        "        posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
        "        self.register_buffer('posterior_variance', posterior_variance)\n",
        "        self.register_buffer('posterior_log_variance_clipped', torch.log(torch.clamp(posterior_variance, min=1e-20)))\n",
        "        self.register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod))\n",
        "        self.register_buffer('posterior_mean_coef2', (1.0 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1.0 - alphas_cumprod))\n",
        "\n",
        "    def _extract(self, a, t, x_shape):\n",
        "        batch_size = t.shape[0]\n",
        "        out = a.gather(-1, t)\n",
        "        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "    def q_sample(self, x_0, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_0)\n",
        "        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_0.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape)\n",
        "        x_t = sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "        return x_t, noise\n",
        "\n",
        "    def predict_start_from_noise(self, x_t, t, noise):\n",
        "        sqrt_recip = self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape)\n",
        "        sqrt_recipm1 = self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
        "        return sqrt_recip * x_t - sqrt_recipm1 * noise\n",
        "\n",
        "    def q_posterior(self, x_0, x_t, t):\n",
        "        mean = self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_0 + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
        "        log_var = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
        "        return mean, log_var\n",
        "\n",
        "    def p_mean_variance(self, x_t, t):\n",
        "        predicted_noise = self.model(x_t, t)\n",
        "        x_0_pred = self.predict_start_from_noise(x_t, t, predicted_noise)\n",
        "        x_0_pred = torch.clamp(x_0_pred, -1.0, 1.0)\n",
        "        model_mean, model_log_variance = self.q_posterior(x_0_pred, x_t, t)\n",
        "        return model_mean, model_log_variance\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(self, x_t, t):\n",
        "        model_mean, model_log_variance = self.p_mean_variance(x_t, t)\n",
        "        noise = torch.randn_like(x_t)\n",
        "        nonzero_mask = (t != 0).float().reshape(-1, *((1,) * (len(x_t.shape) - 1)))\n",
        "        return model_mean + nonzero_mask * torch.exp(0.5 * model_log_variance) * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, batch_size, image_size, channels, progress=True):\n",
        "        device = self.betas.device\n",
        "        x = torch.randn((batch_size, channels, image_size, image_size), device=device)\n",
        "        timesteps = list(reversed(range(self.timesteps)))\n",
        "        if progress:\n",
        "            timesteps = tqdm(timesteps, desc='Sampling', leave=False)\n",
        "        for t in timesteps:\n",
        "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
        "            x = self.p_sample(x, t_batch)\n",
        "        return x\n",
        "\n",
        "    def compute_loss(self, x_0):\n",
        "        b = x_0.shape[0]\n",
        "        t = torch.randint(0, self.timesteps, (b,), device=x_0.device, dtype=torch.long)\n",
        "        noise = torch.randn_like(x_0)\n",
        "        x_t, _ = self.q_sample(x_0, t, noise)\n",
        "        predicted_noise = self.model(x_t, t)\n",
        "        return F.mse_loss(predicted_noise, noise)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d306275d",
      "metadata": {
        "id": "d306275d"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.9999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "    def update(self, model):\n",
        "        for name, param in model.named_parameters():\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "            self.shadow[name] = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
        "    def apply_to(self, model):\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in self.shadow:\n",
        "                param.data.copy_(self.shadow[name])\n",
        "    def state_dict(self):\n",
        "        return {'decay': self.decay, 'shadow': self.shadow}\n",
        "    def load_state_dict(self, state):\n",
        "        if not state:\n",
        "            return\n",
        "        self.decay = state.get('decay', self.decay)\n",
        "        self.shadow = state.get('shadow', self.shadow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78eaac3d",
      "metadata": {
        "id": "78eaac3d"
      },
      "source": [
        "## 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9a105d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d9a105d4",
        "outputId": "c3b315fe-ca85-4838-fe53-75d2bef94bf1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2198567661.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler() if (config.training.mixed_precision and DEVICE.type == 'cuda') else None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 85,459,076\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mis0724rp\u001b[0m (\u001b[33mis0724rp-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260112_091813-smpn2zz1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is0724rp-/ldm-celebahq/runs/smpn2zz1' target=\"_blank\">ldm_celebahq_colab</a></strong> to <a href='https://wandb.ai/is0724rp-/ldm-celebahq' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/is0724rp-/ldm-celebahq' target=\"_blank\">https://wandb.ai/is0724rp-/ldm-celebahq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/is0724rp-/ldm-celebahq/runs/smpn2zz1' target=\"_blank\">https://wandb.ai/is0724rp-/ldm-celebahq/runs/smpn2zz1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/100000 [00:00<?, ?it/s]/tmp/ipython-input-2198567661.py:170: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Training:   5%|▌         | 5001/100000 [13:35<303:54:51, 11.52s/it, loss=0.2894, lr=0.000200]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00005000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  10%|█         | 10001/100000 [27:08<280:46:08, 11.23s/it, loss=0.2742, lr=0.000199]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00010000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  15%|█▌        | 15001/100000 [40:43<272:42:59, 11.55s/it, loss=0.2627, lr=0.000195]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00015000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  20%|██        | 20001/100000 [54:17<255:55:36, 11.52s/it, loss=0.2652, lr=0.000188]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00020000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  25%|██▌       | 25001/100000 [1:07:49<227:02:14, 10.90s/it, loss=0.2574, lr=0.000179]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00025000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  30%|███       | 30001/100000 [1:21:21<211:15:31, 10.86s/it, loss=0.2583, lr=0.000168]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00030000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  35%|███▌      | 35001/100000 [1:34:57<211:46:10, 11.73s/it, loss=0.2539, lr=0.000155]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00035000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  40%|████      | 40001/100000 [1:48:31<191:44:45, 11.50s/it, loss=0.2534, lr=0.000140]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00040000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  45%|████▌     | 45001/100000 [2:02:07<177:21:13, 11.61s/it, loss=0.2573, lr=0.000125]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00045000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  50%|█████     | 50001/100000 [2:15:39<156:25:47, 11.26s/it, loss=0.2526, lr=0.000108]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00050000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  55%|█████▌    | 55001/100000 [2:29:11<135:26:50, 10.84s/it, loss=0.2484, lr=0.000092]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00055000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  60%|██████    | 60001/100000 [2:42:46<125:10:07, 11.27s/it, loss=0.2490, lr=0.000075]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00060000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  65%|██████▌   | 65001/100000 [2:56:21<108:58:18, 11.21s/it, loss=0.2420, lr=0.000060]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00065000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  70%|███████   | 70001/100000 [3:10:00<97:35:41, 11.71s/it, loss=0.2456, lr=0.000045] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00070000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  75%|███████▌  | 75001/100000 [3:23:36<79:24:37, 11.44s/it, loss=0.2473, lr=0.000032] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00075000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  80%|████████  | 80001/100000 [3:37:11<62:25:55, 11.24s/it, loss=0.2455, lr=0.000021]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00080000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  85%|████████▌ | 85001/100000 [3:50:48<48:15:49, 11.58s/it, loss=0.2457, lr=0.000012]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00085000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  90%|█████████ | 90001/100000 [4:04:21<30:11:23, 10.87s/it, loss=0.2419, lr=0.000005]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00090000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  95%|█████████▌| 95001/100000 [4:17:54<14:53:19, 10.72s/it, loss=0.2380, lr=0.000001]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00095000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 100000/100000 [4:31:29<00:00,  6.14it/s, loss=0.2439, lr=0.000000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00100000.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint: /content/drive/MyDrive/research-experiment/LDM/outputs/checkpoints/ckpt_00100000.pt\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>████▆▆▆▅▅▅▄▄▅▄▅▄▄▄▄▄▄▄▃▃▂▃▂▃▂▃▂▂▁▂▃▂▂▁▂▃</td></tr><tr><td>lr</td><td>▃▅▆▇▇████████▇▇▇▇▇▇▆▆▆▆▆▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.24386</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>step</td><td>100000</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ldm_celebahq_colab</strong> at: <a href='https://wandb.ai/is0724rp-/ldm-celebahq/runs/smpn2zz1' target=\"_blank\">https://wandb.ai/is0724rp-/ldm-celebahq/runs/smpn2zz1</a><br> View project at: <a href='https://wandb.ai/is0724rp-/ldm-celebahq' target=\"_blank\">https://wandb.ai/is0724rp-/ldm-celebahq</a><br>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20260112_091813-smpn2zz1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training done.\n"
          ]
        }
      ],
      "source": [
        "model = UNet(config.model).to(DEVICE)\n",
        "diffusion = GaussianDiffusion(model, config.diffusion).to(DEVICE)\n",
        "ema = EMA(model, decay=config.training.ema_decay)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.training.learning_rate)\n",
        "scaler = GradScaler() if (config.training.mixed_precision and DEVICE.type == 'cuda') else None\n",
        "\n",
        "        total_steps = max(1, config.training.total_steps)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=total_steps,\n",
        "            eta_min=2e-6,\n",
        "        )\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'Model parameters: {num_params:,}')\n",
        "\n",
        "OUTPUT_DIR = config.output_dir\n",
        "CHECKPOINT_DIR = Path(config.output_dir) / 'checkpoints'\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def decode_latents(latents):\n",
        "    scale = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
        "    latents = latents / scale\n",
        "    latents = latents.to(dtype=vae.dtype)\n",
        "    images = vae.decode(latents).sample\n",
        "    images = (images + 1) / 2\n",
        "    return images.clamp(0, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def vae_sanity_check(n: int = 8, out_path: str = None):\n",
        "    # VAE-only reconstruction to verify normalization and scaling\n",
        "    loader = get_dataloader(\n",
        "        config.data.dataset,\n",
        "        config.data.image_size,\n",
        "        batch_size=n,\n",
        "        num_workers=config.training.num_workers,\n",
        "        train=False,\n",
        "    )\n",
        "    images, _ = next(iter(loader))\n",
        "    images = images.to(DEVICE)\n",
        "    if config.vae.use_fp16 and DEVICE.type == 'cuda':\n",
        "        images = images.half()\n",
        "\n",
        "    dist = vae.encode(images).latent_dist\n",
        "    latents = dist.sample()\n",
        "    if latents.shape[-1] != config.model.image_size:\n",
        "        print(\n",
        "            f'WARNING: latent size {latents.shape[-1]} != expected {config.model.image_size}'\n",
        "        )\n",
        "\n",
        "    scale = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
        "    latents_scaled = latents * scale\n",
        "    recon = decode_latents(latents_scaled)\n",
        "\n",
        "    images_vis = (images + 1) / 2\n",
        "    images_vis = images_vis.clamp(0, 1)\n",
        "\n",
        "    grid = torch.cat([images_vis, recon], dim=0)\n",
        "    grid = torchvision.utils.make_grid(grid, nrow=n, padding=2)\n",
        "    if out_path is None:\n",
        "        out_path = str(Path(OUTPUT_DIR) / 'vae_recon.png')\n",
        "    torchvision.utils.save_image(grid, out_path)\n",
        "    print(f'Saved VAE sanity check: {out_path}')\n",
        "\n",
        "def log_samples(step):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        latents = diffusion.sample(64, config.model.image_size, config.model.out_channels, progress=False)\n",
        "        images = decode_latents(latents)\n",
        "    grid = torchvision.utils.make_grid(images, nrow=8, padding=2)\n",
        "    save_path = Path(OUTPUT_DIR) / f'samples_{step:08d}.png'\n",
        "    torchvision.utils.save_image(grid, save_path)\n",
        "    if wandb.run is not None:\n",
        "        wandb.log({'samples': wandb.Image(grid), 'step': step})\n",
        "    model.train()\n",
        "\n",
        "def _checkpoint_state(step, running_loss):\n",
        "    return {\n",
        "        'model': model.state_dict(),\n",
        "        'ema': ema.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict(),\n",
        "        'scaler': scaler.state_dict() if scaler is not None else None,\n",
        "        'step': step,\n",
        "        'running_loss': running_loss,\n",
        "    }\n",
        "\n",
        "def save_checkpoint(step, running_loss):\n",
        "    state = _checkpoint_state(step, running_loss)\n",
        "    ckpt_path = CHECKPOINT_DIR / f'ckpt_{step:08d}.pt'\n",
        "    torch.save(state, ckpt_path)\n",
        "    torch.save(state, CHECKPOINT_DIR / 'latest.pt')\n",
        "    print(f'Saved checkpoint: {ckpt_path}')\n",
        "\n",
        "def find_latest_checkpoint():\n",
        "    latest = CHECKPOINT_DIR / 'latest.pt'\n",
        "    if latest.exists():\n",
        "        return latest\n",
        "    candidates = sorted(CHECKPOINT_DIR.glob('ckpt_*.pt'))\n",
        "    return candidates[-1] if candidates else None\n",
        "\n",
        "def load_checkpoint(path):\n",
        "    state = torch.load(path, map_location=DEVICE)\n",
        "    step = int(state.get('step', 0))\n",
        "    running_loss = float(state.get('running_loss', 0.0))\n",
        "    model.load_state_dict(state['model'])\n",
        "    ema.load_state_dict(state.get('ema', {}))\n",
        "    optimizer.load_state_dict(state['optimizer'])\n",
        "    if state.get('scheduler') is not None:\n",
        "        scheduler.load_state_dict(state['scheduler'])\n",
        "    else:\n",
        "        scheduler.last_epoch = step - 1\n",
        "    if scaler is not None and state.get('scaler') is not None:\n",
        "        scaler.load_state_dict(state['scaler'])\n",
        "    print(f'Resumed from checkpoint: {path} (step={step})')\n",
        "    return step, running_loss\n",
        "\n",
        "WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
        "if WANDB_API_KEY:\n",
        "    wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "wandb.init(project=f'ldm-{config.data.dataset}', name=config.exp_name, config={\n",
        "    'dataset': config.data.dataset,\n",
        "    'timesteps': config.diffusion.timesteps,\n",
        "    'beta_schedule': config.diffusion.beta_schedule,\n",
        "    'batch_size': config.training.batch_size,\n",
        "    'lr': config.training.learning_rate,\n",
        "    'image_size': config.data.image_size,\n",
        "    'latent_size': config.model.image_size,\n",
        "    'downsample_factor': config.vae.downsample_factor,\n",
        "    'model_params': num_params,\n",
        "})\n",
        "\n",
        "step = 0\n",
        "running_loss = 0.0\n",
        "latest_ckpt = find_latest_checkpoint()\n",
        "if latest_ckpt is not None:\n",
        "    step, running_loss = load_checkpoint(latest_ckpt)\n",
        "\n",
        "pbar = tqdm(total=config.training.total_steps, initial=step, desc='Training')\n",
        "\n",
        "while step < config.training.total_steps:\n",
        "    for batch in latent_loader:\n",
        "        if step >= config.training.total_steps:\n",
        "            break\n",
        "        latents = batch[0].to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        if scaler is not None:\n",
        "            with autocast():\n",
        "                loss = diffusion.compute_loss(latents)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            loss = diffusion.compute_loss(latents)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "        ema.update(model)\n",
        "        step += 1\n",
        "        running_loss += loss.item()\n",
        "        if step % config.training.log_every == 0:\n",
        "            avg_loss = running_loss / config.training.log_every\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            wandb.log({'loss': avg_loss, 'lr': lr, 'step': step})\n",
        "            pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.6f}'})\n",
        "            running_loss = 0.0\n",
        "        if step % config.training.sample_every == 0:\n",
        "            log_samples(step)\n",
        "        if step % config.training.save_every == 0:\n",
        "            save_checkpoint(step, running_loss)\n",
        "        pbar.update(1)\n",
        "\n",
        "pbar.close()\n",
        "save_checkpoint(step, running_loss)\n",
        "wandb.finish()\n",
        "print('Training done.')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
