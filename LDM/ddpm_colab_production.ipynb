{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e410b0be",
   "metadata": {},
   "source": [
    "# LDM (Latent Diffusion Models) - CelebA-HQ / LSUN-Churches Training\n",
    "\n",
    "Google Colab (T4) で動作する\n",
    "\n",
    "**設定**\n",
    "- Dataset: CelebA-HQ または LSUN-Churches\n",
    "- Resolution: 128px\n",
    "- Downsample Factor: f=4 (潜在空間 32×32)\n",
    "- VAE: diffusers の学習済み AutoencoderKL\n",
    "- Scheduler: cosine\n",
    "- wandb でログと生成画像を記録\n",
    "\n",
    "**なぜ f=4 か**\n",
    "- f=8だと 128÷8=16 で潜在変数が小さすぎる\n",
    "- f=4なら 128÷4=32 で顔の表情や建物の柱までクッキリ出る"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a434b",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6dedb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 12 02:49:19 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   41C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPU確認\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68106ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: ./outputs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 出力ディレクトリ\n",
    "import os\n",
    "os.environ['DIFFUSERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "OUTPUT_DIR = './outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "929c9f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package installation complete!\n"
     ]
    }
   ],
   "source": [
    "# 必要なパッケージ\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q diffusers transformers accelerate\n",
    "!pip install -q einops tqdm scipy pillow wandb gdown\n",
    "print(\"Package installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145ac604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
      "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Config:\n",
      "  Dataset: celebahq\n",
      "  Image size: 128px\n",
      "  Downsample factor: 4\n",
      "  Latent size: 32x32\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Literal, Optional, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    image_size: int = 32  # 128 / 4 = 32 latent size\n",
    "    in_channels: int = 4\n",
    "    out_channels: int = 4\n",
    "    model_channels: int = 128  # Larger for 32x32 latent\n",
    "    channel_mult: tuple = (1, 2, 3, 4)  # More levels for 32x32\n",
    "    num_res_blocks: int = 2\n",
    "    attention_resolutions: tuple = (16, 8)  # Attention at 16x16 and 8x8\n",
    "    dropout: float = 0.1\n",
    "    num_heads: int = 8\n",
    "    use_scale_shift_norm: bool = True\n",
    "\n",
    "@dataclass\n",
    "class DiffusionConfig:\n",
    "    timesteps: int = 1000\n",
    "    beta_schedule: Literal['linear', 'cosine', 'quadratic'] = 'cosine'\n",
    "    beta_start: float = 1e-4\n",
    "    beta_end: float = 0.02\n",
    "    s: float = 0.008\n",
    "\n",
    "@dataclass\n",
    "class VAEConfig:\n",
    "    model_id: str = 'stabilityai/sd-vae-ft-mse'\n",
    "    subfolder: Optional[str] = None\n",
    "    downsample_factor: int = 4  # f=4 for 128px -> 32x32 latent\n",
    "    latent_channels: int = 4\n",
    "    latent_scaling_factor: float = 0.18215\n",
    "    cache_dir: str = './data/latents'\n",
    "    use_fp16: bool = True\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    dataset: str = 'celebahq'  # 'celebahq' or 'lsun_churches'\n",
    "    data_dir: str = './data'\n",
    "    image_size: int = 128  # 128px input\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    batch_size: int = 64  # Smaller for 128px\n",
    "    learning_rate: float = 2e-4\n",
    "    total_steps: int = 100_000\n",
    "    warmup_steps: int = 5000\n",
    "    grad_clip: float = 1.0\n",
    "    ema_decay: float = 0.9999\n",
    "    save_every: int = 5000\n",
    "    sample_every: int = 5000\n",
    "    log_every: int = 100\n",
    "    num_workers: int = 2\n",
    "    mixed_precision: bool = True\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    diffusion: DiffusionConfig = field(default_factory=DiffusionConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    vae: VAEConfig = field(default_factory=VAEConfig)\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    output_dir: str = './outputs'\n",
    "    exp_name: str = 'ldm_celebahq_colab'\n",
    "    seed: int = 42\n",
    "\n",
    "config = Config()\n",
    "config.model.image_size = config.data.image_size // config.vae.downsample_factor\n",
    "config.model.in_channels = config.vae.latent_channels\n",
    "config.model.out_channels = config.vae.latent_channels\n",
    "\n",
    "set_seed(config.seed)\n",
    "print('Config:')\n",
    "print(f'  Dataset: {config.data.dataset}')\n",
    "print(f'  Image size: {config.data.image_size}px')\n",
    "print(f'  Downsample factor: {config.vae.downsample_factor}')\n",
    "print(f'  Latent size: {config.model.image_size}x{config.model.image_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0634597",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1175724655.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Google Drive を使う場合はここを有効化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "# Google Drive を使う場合はここを有効化\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path('/content/drive/MyDrive/research-experiment/LDM/data')\n",
    "OUTPUT_ROOT = Path('/content/drive/MyDrive/research-experiment/LDM/outputs')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "config.data.data_dir = str(DATA_ROOT)\n",
    "config.output_dir = str(OUTPUT_ROOT)\n",
    "OUTPUT_DIR = config.output_dir\n",
    "print('Data dir:', config.data.data_dir)\n",
    "print('Output dir:', config.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ac604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
      "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-851596971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0mlatent_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_latents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_recompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0mlatent_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-851596971.py\u001b[0m in \u001b[0;36mload_vae\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_fp16\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoencoderKL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "def download_celebahq():\n",
    "    \"\"\"Download CelebA-HQ dataset (subset for training).\"\"\"\n",
    "    drive_dir = Path(config.data.data_dir) / 'celebahq'\n",
    "    drive_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    local_dir = Path('/content') / drive_dir.name\n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _count_images(p):\n",
    "        return len(list(p.glob('*.jpg'))) + len(list(p.glob('*.png')))\n",
    "\n",
    "    local_count = _count_images(local_dir)\n",
    "    if local_count > 1000:\n",
    "        print(f'CelebA-HQ already extracted locally: {local_count} images')\n",
    "        return local_dir\n",
    "\n",
    "    drive_count = _count_images(drive_dir)\n",
    "    if drive_count > 1000:\n",
    "        print(f'CelebA-HQ already on Drive: {drive_count} images')\n",
    "        return drive_dir\n",
    "\n",
    "    print('Downloading CelebA-HQ dataset...')\n",
    "    # Using a publicly available CelebA-HQ 256x256 subset\n",
    "    # You can replace this with your own source\n",
    "    url = 'https://drive.google.com/uc?id=1badu11NqxGf6qM3PTTooQDJvQbejgbTv'\n",
    "    output = drive_dir / 'celeba_hq.zip'\n",
    "\n",
    "    try:\n",
    "        gdown.download(url, str(output), quiet=False)\n",
    "        print('Extracting to /content...')\n",
    "        with zipfile.ZipFile(output, 'r') as z:\n",
    "            members = z.namelist()\n",
    "            for member in tqdm(members, desc='Extracting', unit='file'):\n",
    "                z.extract(member, local_dir)\n",
    "        print('Saving extracted files to Drive...')\n",
    "        files = [p for p in local_dir.rglob('*') if p.is_file()]\n",
    "        for f in tqdm(files, desc='Copying to Drive', unit='file'):\n",
    "            rel = f.relative_to(local_dir)\n",
    "            dst = drive_dir / rel\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(f, dst)\n",
    "        output.unlink()  # Remove zip\n",
    "        print('CelebA-HQ download complete!')\n",
    "    except Exception as e:\n",
    "        print(f'Download failed: {e}')\n",
    "        print('Please manually download CelebA-HQ and place images in:', drive_dir)\n",
    "        print('Alternative: Use LSUN Churches by changing config.data.dataset to \"lsun_churches\"')\n",
    "\n",
    "    return local_dir if local_dir.exists() else drive_dir\n",
    "\n",
    "\n",
    "def download_lsun_churches():\n",
    "    \"\"\"Download LSUN Churches dataset (subset for training).\"\"\"\n",
    "    data_dir = Path(config.data.data_dir) / 'lsun_churches'\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    existing_images = list(data_dir.glob('*.jpg')) + list(data_dir.glob('*.webp')) + list(data_dir.glob('*.png'))\n",
    "    if len(existing_images) > 1000:\n",
    "        print(f'LSUN Churches already downloaded: {len(existing_images)} images')\n",
    "        return data_dir\n",
    "\n",
    "    print('Downloading LSUN Churches dataset...')\n",
    "    print('Note: Full LSUN is very large. Using a smaller subset for demo.')\n",
    "    # You can replace this with your own LSUN source\n",
    "    print('Please manually download LSUN Churches and place images in:', data_dir)\n",
    "    print('Or switch to CelebA-HQ by changing config.data.dataset to \"celebahq\"')\n",
    "\n",
    "    return data_dir\n",
    "\n",
    "# Generic ImageFolder dataset\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, transform=None, extensions=('.jpg', '.jpeg', '.png', '.webp')):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.extensions = extensions\n",
    "        self.image_paths = self._find_images()\n",
    "        print(f'Found {len(self.image_paths)} images in {data_dir}')\n",
    "\n",
    "    def _find_images(self):\n",
    "        image_paths = []\n",
    "        for ext in self.extensions:\n",
    "            image_paths.extend(self.data_dir.rglob(f'*{ext}'))\n",
    "            image_paths.extend(self.data_dir.rglob(f'*{ext.upper()}'))\n",
    "        return sorted(image_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # Dummy label\n",
    "\n",
    "def get_dataloader(dataset_name: str, image_size: int, batch_size: int, num_workers: int, train: bool = True):\n",
    "    \"\"\"Get dataloader for CelebA-HQ or LSUN Churches.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.RandomHorizontalFlip() if train else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    if dataset_name == 'celebahq':\n",
    "        data_dir = download_celebahq()\n",
    "    elif dataset_name == 'lsun_churches':\n",
    "        data_dir = download_lsun_churches()\n",
    "    else:\n",
    "        raise ValueError(f'Unknown dataset: {dataset_name}')\n",
    "\n",
    "    dataset = ImageFolderDataset(str(data_dir), transform=transform)\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(f'No images found in {data_dir}')\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=train, num_workers=num_workers, drop_last=train)\n",
    "\n",
    "def load_vae():\n",
    "    dtype = torch.float16 if (config.vae.use_fp16 and DEVICE.type == 'cuda') else torch.float32\n",
    "    vae = AutoencoderKL.from_pretrained(config.vae.model_id, subfolder=config.vae.subfolder)\n",
    "    vae = vae.to(DEVICE, dtype=dtype).eval()\n",
    "    for p in vae.parameters():\n",
    "        p.requires_grad = False\n",
    "    return vae\n",
    "\n",
    "@torch.no_grad()\n",
    "def prepare_latents(vae, force_recompute: bool = False):\n",
    "    cache_dir = Path(config.vae.cache_dir)\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    safe_id = config.vae.model_id.replace('/', '_')\n",
    "    cache_path = cache_dir / f\"{config.data.dataset}_{config.data.image_size}_f{config.vae.downsample_factor}_{safe_id}_latents.pt\"\n",
    "\n",
    "    if cache_path.exists() and not force_recompute:\n",
    "        payload = torch.load(cache_path, map_location='cpu')\n",
    "        dataset = TensorDataset(payload['latents'], payload['labels'])\n",
    "        print(f'Loaded cached latents: {cache_path}')\n",
    "        print(f'  Shape: {payload[\"latents\"].shape}')\n",
    "        return dataset\n",
    "\n",
    "    loader = get_dataloader(config.data.dataset, config.data.image_size, config.training.batch_size, config.training.num_workers, train=True)\n",
    "\n",
    "    print(f'Encoding latents for {config.data.dataset} at {config.data.image_size}px...')\n",
    "    print(f'  VAE: {config.vae.model_id}')\n",
    "    print(f'  Downsample factor: {config.vae.downsample_factor}')\n",
    "    print(f'  Expected latent size: {config.model.image_size}x{config.model.image_size}')\n",
    "\n",
    "    latents_list, labels_list = [], []\n",
    "    for images, labels in tqdm(loader, desc='Encoding latents'):\n",
    "        images = images.to(DEVICE)\n",
    "        if config.vae.use_fp16 and DEVICE.type == 'cuda':\n",
    "            images = images.half()\n",
    "        scale = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
    "        dist = vae.encode(images).latent_dist\n",
    "        latents = dist.sample() * scale\n",
    "        latents_list.append(latents.cpu())\n",
    "        labels_list.append(labels.cpu())\n",
    "\n",
    "    latents = torch.cat(latents_list, dim=0)\n",
    "    labels = torch.cat(labels_list, dim=0)\n",
    "    torch.save({'latents': latents, 'labels': labels}, cache_path)\n",
    "    print(f'Saved latents to: {cache_path}')\n",
    "    print(f'  Shape: {latents.shape}')\n",
    "    return TensorDataset(latents, labels)\n",
    "\n",
    "vae = load_vae()\n",
    "# Sync config with actual VAE settings (fix downsample mismatch + scaling)\n",
    "config.vae.latent_scaling_factor = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
    "config.vae.downsample_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "config.model.image_size = config.data.image_size // config.vae.downsample_factor\n",
    "config.model.in_channels = vae.config.latent_channels\n",
    "config.model.out_channels = vae.config.latent_channels\n",
    "\n",
    "print(f'VAE scaling_factor: {config.vae.latent_scaling_factor}')\n",
    "print(f'VAE downsample_factor: {config.vae.downsample_factor}')\n",
    "print(f'Latent size: {config.model.image_size}x{config.model.image_size}')\n",
    "latent_dataset = prepare_latents(vae, force_recompute=False)\n",
    "latent_loader = DataLoader(latent_dataset, batch_size=config.training.batch_size, shuffle=True, num_workers=config.training.num_workers, drop_last=True)\n",
    "print(f'Latent batches: {len(latent_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a40880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "def download_celebahq():\n",
    "    \"\"\"Download CelebA-HQ dataset (subset for training).\"\"\"\n",
    "    drive_dir = Path(config.data.data_dir) / 'celeba_hq_256'\n",
    "    drive_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    local_dir = Path('/content') / drive_dir.name\n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _count_images(p):\n",
    "        return len(list(p.glob('*.jpg'))) + len(list(p.glob('*.png')))\n",
    "\n",
    "    local_count = _count_images(local_dir)\n",
    "    if local_count > 1000:\n",
    "        print(f'CelebA-HQ already extracted locally: {local_count} images')\n",
    "        return local_dir\n",
    "\n",
    "    drive_count = _count_images(drive_dir)\n",
    "    if drive_count > 1000:\n",
    "        print(f'CelebA-HQ already on Drive: {drive_count} images')\n",
    "        return drive_dir\n",
    "\n",
    "    print('Downloading CelebA-HQ dataset...')\n",
    "    # Using a publicly available CelebA-HQ 256x256 subset\n",
    "    # You can replace this with your own source\n",
    "    url = 'https://drive.google.com/uc?id=1badu11NqxGf6qM3PTTooQDJvQbejgbTv'\n",
    "    output = drive_dir / 'celeba_hq.zip'\n",
    "\n",
    "    try:\n",
    "        gdown.download(url, str(output), quiet=False)\n",
    "        print('Extracting to /content...')\n",
    "        with zipfile.ZipFile(output, 'r') as z:\n",
    "            members = z.namelist()\n",
    "            for member in tqdm(members, desc='Extracting', unit='file'):\n",
    "                z.extract(member, local_dir)\n",
    "        print('Saving extracted files to Drive...')\n",
    "        files = [p for p in local_dir.rglob('*') if p.is_file()]\n",
    "        for f in tqdm(files, desc='Copying to Drive', unit='file'):\n",
    "            rel = f.relative_to(local_dir)\n",
    "            dst = drive_dir / rel\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(f, dst)\n",
    "        output.unlink()  # Remove zip\n",
    "        print('CelebA-HQ download complete!')\n",
    "    except Exception as e:\n",
    "        print(f'Download failed: {e}')\n",
    "        print('Please manually download CelebA-HQ and place images in:', drive_dir)\n",
    "        print('Alternative: Use LSUN Churches by changing config.data.dataset to \"lsun_churches\"')\n",
    "\n",
    "    return local_dir if local_dir.exists() else drive_dir\n",
    "\n",
    "\n",
    "def download_lsun_churches():\n",
    "    \"\"\"Download LSUN Churches dataset (subset for training).\"\"\"\n",
    "    data_dir = Path(config.data.data_dir) / 'lsun_churches'\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    existing_images = list(data_dir.glob('*.jpg')) + list(data_dir.glob('*.webp')) + list(data_dir.glob('*.png'))\n",
    "    if len(existing_images) > 1000:\n",
    "        print(f'LSUN Churches already downloaded: {len(existing_images)} images')\n",
    "        return data_dir\n",
    "\n",
    "    print('Downloading LSUN Churches dataset...')\n",
    "    print('Note: Full LSUN is very large. Using a smaller subset for demo.')\n",
    "    # You can replace this with your own LSUN source\n",
    "    print('Please manually download LSUN Churches and place images in:', data_dir)\n",
    "    print('Or switch to CelebA-HQ by changing config.data.dataset to \"celebahq\"')\n",
    "\n",
    "    return data_dir\n",
    "\n",
    "# Generic ImageFolder dataset\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, transform=None, extensions=('.jpg', '.jpeg', '.png', '.webp')):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.extensions = extensions\n",
    "        self.image_paths = self._find_images()\n",
    "        print(f'Found {len(self.image_paths)} images in {data_dir}')\n",
    "\n",
    "    def _find_images(self):\n",
    "        image_paths = []\n",
    "        for ext in self.extensions:\n",
    "            image_paths.extend(self.data_dir.rglob(f'*{ext}'))\n",
    "            image_paths.extend(self.data_dir.rglob(f'*{ext.upper()}'))\n",
    "        return sorted(image_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # Dummy label\n",
    "\n",
    "def get_dataloader(dataset_name: str, image_size: int, batch_size: int, num_workers: int, train: bool = True):\n",
    "    \"\"\"Get dataloader for CelebA-HQ or LSUN Churches.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.RandomHorizontalFlip() if train else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    if dataset_name == 'celebahq':\n",
    "        data_dir = download_celebahq()\n",
    "    elif dataset_name == 'lsun_churches':\n",
    "        data_dir = download_lsun_churches()\n",
    "    else:\n",
    "        raise ValueError(f'Unknown dataset: {dataset_name}')\n",
    "\n",
    "    dataset = ImageFolderDataset(str(data_dir), transform=transform)\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(f'No images found in {data_dir}')\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=train, num_workers=num_workers, drop_last=train)\n",
    "\n",
    "def load_vae():\n",
    "    dtype = torch.float16 if (config.vae.use_fp16 and DEVICE.type == 'cuda') else torch.float32\n",
    "    vae = AutoencoderKL.from_pretrained(config.vae.model_id, subfolder=config.vae.subfolder)\n",
    "    vae = vae.to(DEVICE, dtype=dtype).eval()\n",
    "    for p in vae.parameters():\n",
    "        p.requires_grad = False\n",
    "    return vae\n",
    "\n",
    "@torch.no_grad()\n",
    "def prepare_latents(vae, force_recompute: bool = False):\n",
    "    cache_dir = Path(config.vae.cache_dir)\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    safe_id = config.vae.model_id.replace('/', '_')\n",
    "    cache_path = cache_dir / f\"{config.data.dataset}_{config.data.image_size}_f{config.vae.downsample_factor}_{safe_id}_latents.pt\"\n",
    "\n",
    "    if cache_path.exists() and not force_recompute:\n",
    "        payload = torch.load(cache_path, map_location='cpu')\n",
    "        dataset = TensorDataset(payload['latents'], payload['labels'])\n",
    "        print(f'Loaded cached latents: {cache_path}')\n",
    "        print(f'  Shape: {payload[\"latents\"].shape}')\n",
    "        return dataset\n",
    "\n",
    "    loader = get_dataloader(config.data.dataset, config.data.image_size, config.training.batch_size, config.training.num_workers, train=True)\n",
    "\n",
    "    print(f'Encoding latents for {config.data.dataset} at {config.data.image_size}px...')\n",
    "    print(f'  VAE: {config.vae.model_id}')\n",
    "    print(f'  Downsample factor: {config.vae.downsample_factor}')\n",
    "    print(f'  Expected latent size: {config.model.image_size}x{config.model.image_size}')\n",
    "\n",
    "    latents_list, labels_list = [], []\n",
    "    for images, labels in tqdm(loader, desc='Encoding latents'):\n",
    "        images = images.to(DEVICE)\n",
    "        if config.vae.use_fp16 and DEVICE.type == 'cuda':\n",
    "            images = images.half()\n",
    "        scale = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
    "        dist = vae.encode(images).latent_dist\n",
    "        latents = dist.sample() * scale\n",
    "        latents_list.append(latents.cpu())\n",
    "        labels_list.append(labels.cpu())\n",
    "\n",
    "    latents = torch.cat(latents_list, dim=0)\n",
    "    labels = torch.cat(labels_list, dim=0)\n",
    "    torch.save({'latents': latents, 'labels': labels}, cache_path)\n",
    "    print(f'Saved latents to: {cache_path}')\n",
    "    print(f'  Shape: {latents.shape}')\n",
    "    return TensorDataset(latents, labels)\n",
    "\n",
    "vae = load_vae()\n",
    "# Sync config with actual VAE settings (fix downsample mismatch + scaling)\n",
    "config.vae.latent_scaling_factor = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
    "config.vae.downsample_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "config.model.image_size = config.data.image_size // config.vae.downsample_factor\n",
    "config.model.in_channels = vae.config.latent_channels\n",
    "config.model.out_channels = vae.config.latent_channels\n",
    "\n",
    "print(f'VAE scaling_factor: {config.vae.latent_scaling_factor}')\n",
    "print(f'VAE downsample_factor: {config.vae.downsample_factor}')\n",
    "print(f'Latent size: {config.model.image_size}x{config.model.image_size}')\n",
    "latent_dataset = prepare_latents(vae, force_recompute=False)\n",
    "latent_loader = DataLoader(latent_dataset, batch_size=config.training.batch_size, shuffle=True, num_workers=config.training.num_workers, drop_last=True)\n",
    "print(f'Latent batches: {len(latent_loader)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a2ba6",
   "metadata": {},
   "source": [
    "## 2. Imports & Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f314ef7",
   "metadata": {},
   "source": [
    "## 3. Dataset Download & Dataloader\n",
    "\n",
    "CelebA-HQ または LSUN-Churches をダウンロードします。\n",
    "どちらを使うかは上のConfigで `config.data.dataset` を変更してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d09bc",
   "metadata": {},
   "source": [
    "## 4. UNet (Latent Space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, time: torch.Tensor) -> torch.Tensor:\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class GroupNorm32(nn.GroupNorm):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "def normalization(channels: int, num_groups: int = 32) -> nn.Module:\n",
    "    return GroupNorm32(min(num_groups, channels), channels)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim, dropout=0.1, use_scale_shift_norm=True):\n",
    "        super().__init__()\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "        self.norm1 = normalization(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.norm2 = normalization(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        time_out_dim = out_channels * 2 if use_scale_shift_norm else out_channels\n",
    "        self.time_mlp = nn.Sequential(Swish(), nn.Linear(time_emb_dim, time_out_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "    def forward(self, x, time_emb):\n",
    "        h = self.norm1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv1(h)\n",
    "        time_emb = self.time_mlp(time_emb)\n",
    "        time_emb = time_emb[:, :, None, None]\n",
    "        if self.use_scale_shift_norm:\n",
    "            scale, shift = time_emb.chunk(2, dim=1)\n",
    "            h = self.norm2(h) * (1 + scale) + shift\n",
    "        else:\n",
    "            h = h + time_emb\n",
    "            h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels: int, num_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = channels // num_heads\n",
    "        self.norm = normalization(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x_norm = self.norm(x)\n",
    "        qkv = self.qkv(x_norm)\n",
    "        qkv = qkv.reshape(b, 3, self.num_heads, self.head_dim, h * w)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
    "        q = q.permute(0, 1, 3, 2)\n",
    "        k = k.permute(0, 1, 3, 2)\n",
    "        v = v.permute(0, 1, 3, 2)\n",
    "        attn = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.permute(0, 1, 3, 2).reshape(b, c, h, w)\n",
    "        out = self.proj(out)\n",
    "        return x + out\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, stride=2, padding=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, cfg: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.image_size = cfg.image_size\n",
    "        time_emb_dim = cfg.model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(cfg.model_channels),\n",
    "            nn.Linear(cfg.model_channels, time_emb_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "        self.init_conv = nn.Conv2d(cfg.in_channels, cfg.model_channels, 3, padding=1)\n",
    "\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "\n",
    "        channels = [cfg.model_channels]\n",
    "        ch = cfg.model_channels\n",
    "        resolution = cfg.image_size\n",
    "\n",
    "        for level, mult in enumerate(cfg.channel_mult):\n",
    "            out_ch = cfg.model_channels * mult\n",
    "            for _ in range(cfg.num_res_blocks):\n",
    "                layers = [ResidualBlock(ch, out_ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm)]\n",
    "                ch = out_ch\n",
    "                if resolution in cfg.attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, cfg.num_heads))\n",
    "                self.downs.append(nn.ModuleList(layers))\n",
    "                channels.append(ch)\n",
    "            if level != len(cfg.channel_mult) - 1:\n",
    "                self.downs.append(nn.ModuleList([Downsample(ch)]))\n",
    "                channels.append(ch)\n",
    "                resolution //= 2\n",
    "\n",
    "        self.mid = nn.ModuleList([\n",
    "            ResidualBlock(ch, ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm),\n",
    "            AttentionBlock(ch, cfg.num_heads),\n",
    "            ResidualBlock(ch, ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm),\n",
    "        ])\n",
    "\n",
    "        for level, mult in enumerate(reversed(cfg.channel_mult)):\n",
    "            out_ch = cfg.model_channels * mult\n",
    "            for i in range(cfg.num_res_blocks + 1):\n",
    "                skip_ch = channels.pop()\n",
    "                layers = [ResidualBlock(ch + skip_ch, out_ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm)]\n",
    "                ch = out_ch\n",
    "                if resolution in cfg.attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, cfg.num_heads))\n",
    "                if level != len(cfg.channel_mult) - 1 and i == cfg.num_res_blocks:\n",
    "                    layers.append(Upsample(ch))\n",
    "                    resolution *= 2\n",
    "                self.ups.append(nn.ModuleList(layers))\n",
    "\n",
    "        self.final_norm = normalization(ch)\n",
    "        self.final_conv = nn.Conv2d(ch, cfg.out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_embed(t)\n",
    "        h = self.init_conv(x)\n",
    "        hs = [h]\n",
    "        for layers in self.downs:\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, ResidualBlock):\n",
    "                    h = layer(h, t_emb)\n",
    "                else:\n",
    "                    h = layer(h)\n",
    "            hs.append(h)\n",
    "        for layer in self.mid:\n",
    "            if isinstance(layer, ResidualBlock):\n",
    "                h = layer(h, t_emb)\n",
    "            else:\n",
    "                h = layer(h)\n",
    "        for layers in self.ups:\n",
    "            h = torch.cat([h, hs.pop()], dim=1)\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, ResidualBlock):\n",
    "                    h = layer(h, t_emb)\n",
    "                else:\n",
    "                    h = layer(h)\n",
    "        h = self.final_norm(h)\n",
    "        h = F.silu(h)\n",
    "        return self.final_conv(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753c987",
   "metadata": {},
   "source": [
    "## 5. Diffusion + EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb10ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_beta_schedule(timesteps: int, s: float = 0.008) -> torch.Tensor:\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps) / timesteps\n",
    "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clamp(betas, 0.0001, 0.9999)\n",
    "\n",
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, model, cfg: DiffusionConfig):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.timesteps = cfg.timesteps\n",
    "        betas = cosine_beta_schedule(cfg.timesteps, cfg.s)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod - 1))\n",
    "        posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "        self.register_buffer('posterior_variance', posterior_variance)\n",
    "        self.register_buffer('posterior_log_variance_clipped', torch.log(torch.clamp(posterior_variance, min=1e-20)))\n",
    "        self.register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod))\n",
    "        self.register_buffer('posterior_mean_coef2', (1.0 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1.0 - alphas_cumprod))\n",
    "\n",
    "    def _extract(self, a, t, x_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        out = a.gather(-1, t)\n",
    "        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "    def q_sample(self, x_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_0.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape)\n",
    "        x_t = sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        return x_t, noise\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        sqrt_recip = self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape)\n",
    "        sqrt_recipm1 = self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        return sqrt_recip * x_t - sqrt_recipm1 * noise\n",
    "\n",
    "    def q_posterior(self, x_0, x_t, t):\n",
    "        mean = self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_0 + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        log_var = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return mean, log_var\n",
    "\n",
    "    def p_mean_variance(self, x_t, t):\n",
    "        predicted_noise = self.model(x_t, t)\n",
    "        x_0_pred = self.predict_start_from_noise(x_t, t, predicted_noise)\n",
    "        x_0_pred = torch.clamp(x_0_pred, -1.0, 1.0)\n",
    "        model_mean, model_log_variance = self.q_posterior(x_0_pred, x_t, t)\n",
    "        return model_mean, model_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x_t, t):\n",
    "        model_mean, model_log_variance = self.p_mean_variance(x_t, t)\n",
    "        noise = torch.randn_like(x_t)\n",
    "        nonzero_mask = (t != 0).float().reshape(-1, *((1,) * (len(x_t.shape) - 1)))\n",
    "        return model_mean + nonzero_mask * torch.exp(0.5 * model_log_variance) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size, image_size, channels, progress=True):\n",
    "        device = self.betas.device\n",
    "        x = torch.randn((batch_size, channels, image_size, image_size), device=device)\n",
    "        timesteps = list(reversed(range(self.timesteps)))\n",
    "        if progress:\n",
    "            timesteps = tqdm(timesteps, desc='Sampling', leave=False)\n",
    "        for t in timesteps:\n",
    "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            x = self.p_sample(x, t_batch)\n",
    "        return x\n",
    "\n",
    "    def compute_loss(self, x_0):\n",
    "        b = x_0.shape[0]\n",
    "        t = torch.randint(0, self.timesteps, (b,), device=x_0.device, dtype=torch.long)\n",
    "        noise = torch.randn_like(x_0)\n",
    "        x_t, _ = self.q_sample(x_0, t, noise)\n",
    "        predicted_noise = self.model(x_t, t)\n",
    "        return F.mse_loss(predicted_noise, noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "    def update(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            self.shadow[name] = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "    def apply_to(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in self.shadow:\n",
    "                param.data.copy_(self.shadow[name])\n",
    "    def state_dict(self):\n",
    "        return {'decay': self.decay, 'shadow': self.shadow}\n",
    "    def load_state_dict(self, state):\n",
    "        if not state:\n",
    "            return\n",
    "        self.decay = state.get('decay', self.decay)\n",
    "        self.shadow = state.get('shadow', self.shadow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eaac3d",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a105d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(config.model).to(DEVICE)\n",
    "diffusion = GaussianDiffusion(model, config.diffusion).to(DEVICE)\n",
    "ema = EMA(model, decay=config.training.ema_decay)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.training.learning_rate)\n",
    "scaler = GradScaler() if (config.training.mixed_precision and DEVICE.type == 'cuda') else None\n",
    "\n",
    "def lr_lambda(step):\n",
    "    warmup_steps = max(0, config.training.warmup_steps)\n",
    "    total_steps = max(1, config.training.total_steps)\n",
    "    if warmup_steps > 0 and step < warmup_steps:\n",
    "        return step / warmup_steps\n",
    "    decay_steps = max(1, total_steps - warmup_steps)\n",
    "    progress = (step - warmup_steps) / decay_steps\n",
    "    progress = min(max(progress, 0.0), 1.0)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameters: {num_params:,}')\n",
    "\n",
    "OUTPUT_DIR = config.output_dir\n",
    "CHECKPOINT_DIR = Path(config.output_dir) / 'checkpoints'\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def decode_latents(latents):\n",
    "    scale = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
    "    latents = latents / scale\n",
    "    latents = latents.to(dtype=vae.dtype)\n",
    "    images = vae.decode(latents).sample\n",
    "    images = (images + 1) / 2\n",
    "    return images.clamp(0, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def vae_sanity_check(n: int = 8, out_path: str = None):\n",
    "    # VAE-only reconstruction to verify normalization and scaling\n",
    "    loader = get_dataloader(\n",
    "        config.data.dataset,\n",
    "        config.data.image_size,\n",
    "        batch_size=n,\n",
    "        num_workers=config.training.num_workers,\n",
    "        train=False,\n",
    "    )\n",
    "    images, _ = next(iter(loader))\n",
    "    images = images.to(DEVICE)\n",
    "    if config.vae.use_fp16 and DEVICE.type == 'cuda':\n",
    "        images = images.half()\n",
    "\n",
    "    dist = vae.encode(images).latent_dist\n",
    "    latents = dist.sample()\n",
    "    if latents.shape[-1] != config.model.image_size:\n",
    "        print(\n",
    "            f'WARNING: latent size {latents.shape[-1]} != expected {config.model.image_size}'\n",
    "        )\n",
    "\n",
    "    scale = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
    "    latents_scaled = latents * scale\n",
    "    recon = decode_latents(latents_scaled)\n",
    "\n",
    "    images_vis = (images + 1) / 2\n",
    "    images_vis = images_vis.clamp(0, 1)\n",
    "\n",
    "    grid = torch.cat([images_vis, recon], dim=0)\n",
    "    grid = torchvision.utils.make_grid(grid, nrow=n, padding=2)\n",
    "    if out_path is None:\n",
    "        out_path = str(Path(OUTPUT_DIR) / 'vae_recon.png')\n",
    "    torchvision.utils.save_image(grid, out_path)\n",
    "    print(f'Saved VAE sanity check: {out_path}')\n",
    "\n",
    "def log_samples(step):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        latents = diffusion.sample(64, config.model.image_size, config.model.out_channels, progress=False)\n",
    "        images = decode_latents(latents)\n",
    "    grid = torchvision.utils.make_grid(images, nrow=8, padding=2)\n",
    "    save_path = Path(OUTPUT_DIR) / f'samples_{step:08d}.png'\n",
    "    torchvision.utils.save_image(grid, save_path)\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({'samples': wandb.Image(grid), 'step': step})\n",
    "    model.train()\n",
    "\n",
    "def _checkpoint_state(step, running_loss):\n",
    "    return {\n",
    "        'model': model.state_dict(),\n",
    "        'ema': ema.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'scaler': scaler.state_dict() if scaler is not None else None,\n",
    "        'step': step,\n",
    "        'running_loss': running_loss,\n",
    "    }\n",
    "\n",
    "def save_checkpoint(step, running_loss):\n",
    "    state = _checkpoint_state(step, running_loss)\n",
    "    ckpt_path = CHECKPOINT_DIR / f'ckpt_{step:08d}.pt'\n",
    "    torch.save(state, ckpt_path)\n",
    "    torch.save(state, CHECKPOINT_DIR / 'latest.pt')\n",
    "    print(f'Saved checkpoint: {ckpt_path}')\n",
    "\n",
    "def find_latest_checkpoint():\n",
    "    latest = CHECKPOINT_DIR / 'latest.pt'\n",
    "    if latest.exists():\n",
    "        return latest\n",
    "    candidates = sorted(CHECKPOINT_DIR.glob('ckpt_*.pt'))\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    state = torch.load(path, map_location=DEVICE)\n",
    "    step = int(state.get('step', 0))\n",
    "    running_loss = float(state.get('running_loss', 0.0))\n",
    "    model.load_state_dict(state['model'])\n",
    "    ema.load_state_dict(state.get('ema', {}))\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    if state.get('scheduler') is not None:\n",
    "        scheduler.load_state_dict(state['scheduler'])\n",
    "    else:\n",
    "        scheduler.last_epoch = step - 1\n",
    "    if scaler is not None and state.get('scaler') is not None:\n",
    "        scaler.load_state_dict(state['scaler'])\n",
    "    print(f'Resumed from checkpoint: {path} (step={step})')\n",
    "    return step, running_loss\n",
    "\n",
    "# Colab Secrets (左の🗑から WANDB_API_KEY を登録済み前提)\n",
    "WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
    "if WANDB_API_KEY:\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "wandb.init(project=f'ldm-{config.data.dataset}', name=config.exp_name, config={\n",
    "    'dataset': config.data.dataset,\n",
    "    'timesteps': config.diffusion.timesteps,\n",
    "    'beta_schedule': config.diffusion.beta_schedule,\n",
    "    'batch_size': config.training.batch_size,\n",
    "    'lr': config.training.learning_rate,\n",
    "    'image_size': config.data.image_size,\n",
    "    'latent_size': config.model.image_size,\n",
    "    'downsample_factor': config.vae.downsample_factor,\n",
    "    'model_params': num_params,\n",
    "})\n",
    "\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "latest_ckpt = find_latest_checkpoint()\n",
    "if latest_ckpt is not None:\n",
    "    step, running_loss = load_checkpoint(latest_ckpt)\n",
    "\n",
    "pbar = tqdm(total=config.training.total_steps, initial=step, desc='Training')\n",
    "\n",
    "while step < config.training.total_steps:\n",
    "    for batch in latent_loader:\n",
    "        if step >= config.training.total_steps:\n",
    "            break\n",
    "        latents = batch[0].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                loss = diffusion.compute_loss(latents)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            loss = diffusion.compute_loss(latents)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        ema.update(model)\n",
    "        step += 1\n",
    "        running_loss += loss.item()\n",
    "        if step % config.training.log_every == 0:\n",
    "            avg_loss = running_loss / config.training.log_every\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            wandb.log({'loss': avg_loss, 'lr': lr, 'step': step})\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.6f}'})\n",
    "            running_loss = 0.0\n",
    "        if step % config.training.sample_every == 0:\n",
    "            log_samples(step)\n",
    "        if step % config.training.save_every == 0:\n",
    "            save_checkpoint(step, running_loss)\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "save_checkpoint(step, running_loss)\n",
    "wandb.finish()\n",
    "print('Training done.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
