{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e410b0be",
   "metadata": {
    "id": "e410b0be"
   },
   "source": [
    "# LDM (Latent Diffusion Models) - CelebA-HQ / LSUN-Churches Training\n",
    "\n",
    "Google Colab (T4) で動作する\n",
    "\n",
    "**設定**\n",
    "- Dataset: CelebA-HQ または LSUN-Churches\n",
    "- Resolution: 256px\n",
    "- Downsample Factor: f=4 (潜在空間 64×64)\n",
    "- VAE: diffusers の学習済み AutoencoderKL\n",
    "- Scheduler: cosine\n",
    "- wandb でログと生成画像を記録\n",
    "\n",
    "**なぜ f=4 か**\n",
    "- f=8だと 256÷8=32 で潜在変数が小さすぎる\n",
    "- f=4なら 256÷4=64 で顔の表情や建物の柱までクッキリ出る"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a434b",
   "metadata": {
    "id": "f97a434b"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6dedb9c",
   "metadata": {
    "id": "e6dedb9c",
    "outputId": "b1c613f5-5bc7-48a3-9291-429ae6863bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 26 13:00:11 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          Off |   00000000:04:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPU確認\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68106ec",
   "metadata": {
    "id": "e68106ec",
    "outputId": "3788b63f-d195-449b-ad1f-31d3ae9c1357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: ./outputs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 出力ディレクトリ\n",
    "import os\n",
    "os.environ['DIFFUSERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "OUTPUT_DIR = './outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929c9f92",
   "metadata": {
    "id": "929c9f92",
    "outputId": "efa11c84-e00b-4774-80c0-11ab2ba577cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package installation complete!\n"
     ]
    }
   ],
   "source": [
    "# 必要なパッケージ\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q diffusers transformers accelerate\n",
    "!pip install -q einops tqdm scipy pillow wandb gdown\n",
    "print(\"Package installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ac604",
   "metadata": {
    "id": "145ac604",
    "outputId": "709e9f93-529b-494f-8fc1-263b657a89fa"
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Literal, Optional, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    image_size: int = 32\n",
    "    in_channels: int = 4\n",
    "    out_channels: int = 4\n",
    "    model_channels: int = 128\n",
    "    channel_mult: tuple = (1, 2, 3, 4)\n",
    "    num_res_blocks: int = 2\n",
    "    num_heads: int = 8\n",
    "    use_scale_shift_norm: bool = True\n",
    "    dropout: float = 0.1\n",
    "    attention_resolutions: tuple = (32, 16, 8)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiffusionConfig:\n",
    "    timesteps: int = 1000\n",
    "    beta_schedule: Literal['linear', 'cosine', 'quadratic'] = 'cosine'\n",
    "    beta_start: float = 1e-4\n",
    "    beta_end: float = 0.02\n",
    "    s: float = 0.008\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VAEConfig:\n",
    "    model_id: str = 'stabilityai/sd-vae-ft-mse'\n",
    "    subfolder: Optional[str] = None\n",
    "    downsample_factor: int = 4\n",
    "    latent_channels: int = 4\n",
    "    latent_scaling_factor: float = 0.18215\n",
    "    cache_dir: str = './data/latents'\n",
    "    use_fp16: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    dataset: str = 'celebahq'\n",
    "    data_dir: str = './data'\n",
    "    image_size: int = 256\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    batch_size: int = 64\n",
    "    learning_rate: float = 2e-4\n",
    "    total_steps: int = 100_000\n",
    "    grad_clip: float = 1.0\n",
    "    ema_decay: float = 0.9999\n",
    "    save_every: int = 5000\n",
    "    sample_every: int = 5000\n",
    "    log_every: int = 100\n",
    "    num_workers: int = 2\n",
    "    mixed_precision: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    diffusion: DiffusionConfig = field(default_factory=DiffusionConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    vae: VAEConfig = field(default_factory=VAEConfig)\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    output_dir: str = './outputs'\n",
    "    exp_name: str = 'ldm_celebahq_colab'\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "def _model_config_to_dict(cfg: ModelConfig) -> dict:\n",
    "    data = asdict(cfg)\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, tuple):\n",
    "            data[key] = list(value)\n",
    "    return data\n",
    "\n",
    "\n",
    "config = Config()\n",
    "config.model.image_size = config.data.image_size // config.vae.downsample_factor\n",
    "config.model.in_channels = config.vae.latent_channels\n",
    "config.model.out_channels = config.vae.latent_channels\n",
    "\n",
    "MODEL_CONFIG_METADATA = _model_config_to_dict(config.model)\n",
    "MODEL_CONFIG_SIGNATURE = hashlib.sha1(\n",
    "    json.dumps(MODEL_CONFIG_METADATA, sort_keys=True).encode('utf-8')\n",
    ").hexdigest()[:8]\n",
    "\n",
    "set_seed(config.seed)\n",
    "print('Config:')\n",
    "print(f'  Dataset: {config.data.dataset}')\n",
    "print(f'  Image size: {config.data.image_size}px')\n",
    "print(f'  Downsample factor: {config.vae.downsample_factor}')\n",
    "print(f'  Latent size: {config.model.image_size}x{config.model.image_size}')\n",
    "print(f'  Model config signature: {MODEL_CONFIG_SIGNATURE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c373dce9",
   "metadata": {
    "id": "data_and_vae_setup"
   },
   "outputs": [],
   "source": [
    "\n",
    "import gdown\n",
    "import zipfile\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "def download_celebahq():\n",
    "    \"\"\"Resolve CelebA-HQ directory, preferring Drive cache if available.\"\"\"\n",
    "    drive_root = Path('/content/drive/MyDrive')\n",
    "    drive_repo_candidates = [\n",
    "        drive_root / 'research-experiment' / 'LDM' / 'data' / 'celeba_hq_256',\n",
    "        drive_root / 'src' / 'github.com' / 'frinfo702' / 'research-experiment' / 'LDM' / 'data' / 'celeba_hq_256',\n",
    "    ]\n",
    "    drive_candidates = [\n",
    "        drive_root / 'data' / 'LDM' / 'celeba_hq_256',\n",
    "        drive_root / 'data' / 'celeba_hq_256',\n",
    "        drive_root / 'celeba_hq_256',\n",
    "    ]\n",
    "    local_candidates = [\n",
    "        Path(config.data.data_dir) / 'celeba_hq_256',\n",
    "        Path('./data') / 'celeba_hq_256',\n",
    "    ]\n",
    "\n",
    "    def _count_images(p: Path) -> int:\n",
    "        return len(list(p.glob('*.jpg'))) + len(list(p.glob('*.png')))\n",
    "\n",
    "    for cand in drive_repo_candidates + drive_candidates + local_candidates:\n",
    "        if cand.exists() and _count_images(cand) > 1000:\n",
    "            print(f'CelebA-HQ found: {cand}')\n",
    "            return cand\n",
    "\n",
    "    local_dir = local_candidates[0]\n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print('Downloading CelebA-HQ dataset (approx 1.3GB)...')\n",
    "    url = 'https://drive.google.com/uc?id=1badu11NqxGf6qM3PTTooQDJvQbejgbTv'\n",
    "    output = local_dir / 'celeba_hq.zip'\n",
    "    try:\n",
    "        gdown.download(url, str(output), quiet=False)\n",
    "        print('Extracting CelebA-HQ...')\n",
    "        with zipfile.ZipFile(output, 'r') as z:\n",
    "            members = z.namelist()\n",
    "            for member in tqdm(members, desc='Extracting', unit='file'):\n",
    "                z.extract(member, local_dir)\n",
    "        output.unlink(missing_ok=True)\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f'Download failed: {exc}')\n",
    "        print('Please manually place CelebA-HQ images under', local_dir)\n",
    "    return local_dir\n",
    "\n",
    "\n",
    "def download_lsun_churches():\n",
    "    \"\"\"Download (or locate) LSUN Churches subset.\"\"\"\n",
    "    data_dir = Path(config.data.data_dir) / 'lsun_churches'\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    existing_images = list(data_dir.glob('*.jpg')) + list(data_dir.glob('*.webp')) + list(data_dir.glob('*.png'))\n",
    "    if len(existing_images) > 1000:\n",
    "        print(f'LSUN Churches already present: {len(existing_images)} images')\n",
    "        return data_dir\n",
    "    print('Please place LSUN Churches images under', data_dir)\n",
    "    print('Or switch to CelebA-HQ by setting config.data.dataset = \"celebahq\"')\n",
    "    return data_dir\n",
    "\n",
    "\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, transform=None, extensions=('.jpg', '.jpeg', '.png', '.webp')):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.extensions = extensions\n",
    "        self.image_paths = self._find_images()\n",
    "        if len(self.image_paths) == 0:\n",
    "            raise ValueError(f'No images found in {self.data_dir}')\n",
    "        print(f'Found {len(self.image_paths)} images in {self.data_dir}')\n",
    "\n",
    "    def _find_images(self):\n",
    "        paths = []\n",
    "        for ext in self.extensions:\n",
    "            paths.extend(self.data_dir.rglob(f'*{ext}'))\n",
    "            paths.extend(self.data_dir.rglob(f'*{ext.upper()}'))\n",
    "        return sorted(paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0\n",
    "\n",
    "\n",
    "def get_dataloader(dataset_name: str, image_size: int, batch_size: int, num_workers: int, train: bool = True):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.RandomHorizontalFlip() if train else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    if dataset_name == 'celebahq':\n",
    "        data_dir = download_celebahq()\n",
    "    elif dataset_name == 'lsun_churches':\n",
    "        data_dir = download_lsun_churches()\n",
    "    else:\n",
    "        raise ValueError(f'Unknown dataset: {dataset_name}')\n",
    "    dataset = ImageFolderDataset(str(data_dir), transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=train, num_workers=num_workers, drop_last=train)\n",
    "\n",
    "\n",
    "def load_vae():\n",
    "    dtype = torch.float16 if (config.vae.use_fp16 and DEVICE.type == 'cuda') else torch.float32\n",
    "    vae = AutoencoderKL.from_pretrained(config.vae.model_id, subfolder=config.vae.subfolder)\n",
    "    vae = vae.to(DEVICE, dtype=dtype).eval()\n",
    "    for p in vae.parameters():\n",
    "        p.requires_grad = False\n",
    "    return vae\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prepare_latents(vae, force_recompute: bool = False):\n",
    "    cache_dir = Path(config.vae.cache_dir)\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    safe_id = config.vae.model_id.replace('/', '_')\n",
    "    cache_path = cache_dir / f\"{config.data.dataset}_{config.data.image_size}_f{config.vae.downsample_factor}_{safe_id}_latents.pt\"\n",
    "    if cache_path.exists() and not force_recompute:\n",
    "        payload = torch.load(cache_path, map_location='cpu')\n",
    "        dataset = TensorDataset(payload['latents'], payload['labels'])\n",
    "        print(f'Loaded cached latents: {cache_path}')\n",
    "        print(f\"  Shape: {payload['latents'].shape}\")\n",
    "        return dataset\n",
    "    loader = get_dataloader(\n",
    "        config.data.dataset,\n",
    "        config.data.image_size,\n",
    "        config.training.batch_size,\n",
    "        config.training.num_workers,\n",
    "        train=True,\n",
    "    )\n",
    "    print(f'Encoding latents for {config.data.dataset} at {config.data.image_size}px...')\n",
    "    print(f'  VAE: {config.vae.model_id}')\n",
    "    print(f'  Downsample factor: {config.vae.downsample_factor}')\n",
    "    print(f'  Expected latent size: {config.model.image_size}x{config.model.image_size}')\n",
    "    latents_list, labels_list = [], []\n",
    "    for images, labels in tqdm(loader, desc='Encoding latents'):\n",
    "        images = images.to(DEVICE)\n",
    "        if config.vae.use_fp16 and DEVICE.type == 'cuda':\n",
    "            images = images.half()\n",
    "        scale = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
    "        dist = vae.encode(images).latent_dist\n",
    "        latents = dist.sample() * scale\n",
    "        latents_list.append(latents.cpu())\n",
    "        labels_list.append(labels.cpu())\n",
    "    latents = torch.cat(latents_list, dim=0)\n",
    "    labels = torch.cat(labels_list, dim=0)\n",
    "    torch.save({'latents': latents, 'labels': labels}, cache_path)\n",
    "    print(f'Saved latents to: {cache_path}')\n",
    "    print(f'  Shape: {latents.shape}')\n",
    "    return TensorDataset(latents, labels)\n",
    "\n",
    "\n",
    "vae = load_vae()\n",
    "config.vae.latent_scaling_factor = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
    "config.vae.downsample_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "config.model.image_size = config.data.image_size // config.vae.downsample_factor\n",
    "config.model.in_channels = vae.config.latent_channels\n",
    "config.model.out_channels = vae.config.latent_channels\n",
    "\n",
    "print(f'VAE scaling_factor: {config.vae.latent_scaling_factor}')\n",
    "print(f'VAE downsample_factor: {config.vae.downsample_factor}')\n",
    "print(f'Latent size: {config.model.image_size}x{config.model.image_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0634597",
   "metadata": {
    "id": "c0634597",
    "outputId": "d938cdc8-22b4-43c4-89d0-6fa4d307403d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path('/content/drive/MyDrive/research-experiment/LDM/data')\n",
    "OUTPUT_ROOT = Path('/content/drive/MyDrive/research-experiment/LDM/outputs')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "config.data.data_dir = str(DATA_ROOT)\n",
    "config.output_dir = str(OUTPUT_ROOT)\n",
    "OUTPUT_DIR = config.output_dir\n",
    "print('Data dir:', config.data.data_dir)\n",
    "print('Output dir:', config.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vae_check_cell",
   "metadata": {
    "id": "vae_check_cell"
   },
   "outputs": [],
   "source": [
    "# VAE Reconstruction Check (Latent変換前に実行してVAEの品質を確認)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def check_vae_reconstruction(vae, batch_size=2):\n",
    "    # Create a temporary loader for raw images\n",
    "    check_loader = get_dataloader(\n",
    "        config.data.dataset, \n",
    "        config.data.image_size, \n",
    "        batch_size, \n",
    "        num_workers=1, \n",
    "        train=True\n",
    "    )\n",
    "\n",
    "    images, _ = next(iter(check_loader))\n",
    "    images = images.to(DEVICE)\n",
    "    if config.vae.use_fp16 and DEVICE.type == 'cuda':\n",
    "        images = images.half()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dist = vae.encode(images).latent_dist\n",
    "        latents = dist.sample()\n",
    "\n",
    "        recons = vae.decode(latents).sample\n",
    "\n",
    "    images = (images * 0.5 + 0.5).clamp(0, 1).cpu()\n",
    "    recons = (recons * 0.5 + 0.5).clamp(0, 1).cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(batch_size, 2, figsize=(10, 4 * batch_size))\n",
    "    if batch_size == 1: axes = [axes]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        ax_orig = axes[i][0] if batch_size > 1 else axes[0]\n",
    "        ax_recon = axes[i][1] if batch_size > 1 else axes[1]\n",
    "\n",
    "        ax_orig.imshow(images[i].permute(1, 2, 0))\n",
    "        ax_orig.set_title(\"Original\")\n",
    "        ax_orig.axis('off')\n",
    "\n",
    "        ax_recon.imshow(recons[i].permute(1, 2, 0))\n",
    "        ax_recon.set_title(\"Reconstructed\")\n",
    "        ax_recon.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Checking VAE reconstruction quality...\")\n",
    "check_vae_reconstruction(vae, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_latents_cell",
   "metadata": {
    "id": "prepare_latents_cell"
   },
   "outputs": [],
   "source": [
    "latent_dataset = prepare_latents(vae, force_recompute=False)\n",
    "latent_loader = DataLoader(latent_dataset, batch_size=config.training.batch_size, shuffle=True, num_workers=config.training.num_workers, drop_last=True)\n",
    "print(f'Latent batches: {len(latent_loader)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a2ba6",
   "metadata": {
    "id": "298a2ba6"
   },
   "source": [
    "## 2. Imports & Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f314ef7",
   "metadata": {
    "id": "1f314ef7"
   },
   "source": [
    "## 3. Dataset Download & Dataloader\n",
    "\n",
    "CelebA-HQ または LSUN-Churches をダウンロードします。\n",
    "どちらを使うかは上のConfigで `config.data.dataset` を変更してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d09bc",
   "metadata": {
    "id": "b33d09bc"
   },
   "source": [
    "## 4. UNet (Latent Space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95ffb5",
   "metadata": {
    "id": "ff95ffb5"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, time: torch.Tensor) -> torch.Tensor:\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class GroupNorm32(nn.GroupNorm):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "def normalization(channels: int, num_groups: int = 32) -> nn.Module:\n",
    "    return GroupNorm32(min(num_groups, channels), channels)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim, dropout=0.1, use_scale_shift_norm=True):\n",
    "        super().__init__()\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "        self.norm1 = normalization(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.norm2 = normalization(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        time_out_dim = out_channels * 2 if use_scale_shift_norm else out_channels\n",
    "        self.time_mlp = nn.Sequential(Swish(), nn.Linear(time_emb_dim, time_out_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "    def forward(self, x, time_emb):\n",
    "        h = self.norm1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv1(h)\n",
    "        time_emb = self.time_mlp(time_emb)\n",
    "        time_emb = time_emb[:, :, None, None]\n",
    "        if self.use_scale_shift_norm:\n",
    "            scale, shift = time_emb.chunk(2, dim=1)\n",
    "            h = self.norm2(h) * (1 + scale) + shift\n",
    "        else:\n",
    "            h = h + time_emb\n",
    "            h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels: int, num_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = channels // num_heads\n",
    "        self.norm = normalization(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x_norm = self.norm(x)\n",
    "        qkv = self.qkv(x_norm)\n",
    "        qkv = qkv.reshape(b, 3, self.num_heads, self.head_dim, h * w)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
    "        q = q.permute(0, 1, 3, 2)\n",
    "        k = k.permute(0, 1, 3, 2)\n",
    "        v = v.permute(0, 1, 3, 2)\n",
    "        attn = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.permute(0, 1, 3, 2).reshape(b, c, h, w)\n",
    "        out = self.proj(out)\n",
    "        return x + out\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, stride=2, padding=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, cfg: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.image_size = cfg.image_size\n",
    "        time_emb_dim = cfg.model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(cfg.model_channels),\n",
    "            nn.Linear(cfg.model_channels, time_emb_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "        self.init_conv = nn.Conv2d(cfg.in_channels, cfg.model_channels, 3, padding=1)\n",
    "\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "\n",
    "        channels = [cfg.model_channels]\n",
    "        ch = cfg.model_channels\n",
    "        resolution = cfg.image_size\n",
    "\n",
    "        for level, mult in enumerate(cfg.channel_mult):\n",
    "            out_ch = cfg.model_channels * mult\n",
    "            for _ in range(cfg.num_res_blocks):\n",
    "                layers = [ResidualBlock(ch, out_ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm)]\n",
    "                ch = out_ch\n",
    "                if resolution in cfg.attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, cfg.num_heads))\n",
    "                self.downs.append(nn.ModuleList(layers))\n",
    "                channels.append(ch)\n",
    "            if level != len(cfg.channel_mult) - 1:\n",
    "                self.downs.append(nn.ModuleList([Downsample(ch)]))\n",
    "                channels.append(ch)\n",
    "                resolution //= 2\n",
    "\n",
    "        self.mid = nn.ModuleList([\n",
    "            ResidualBlock(ch, ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm),\n",
    "            AttentionBlock(ch, cfg.num_heads),\n",
    "            ResidualBlock(ch, ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm),\n",
    "        ])\n",
    "\n",
    "        for level, mult in enumerate(reversed(cfg.channel_mult)):\n",
    "            out_ch = cfg.model_channels * mult\n",
    "            for i in range(cfg.num_res_blocks + 1):\n",
    "                skip_ch = channels.pop()\n",
    "                layers = [ResidualBlock(ch + skip_ch, out_ch, time_emb_dim, cfg.dropout, cfg.use_scale_shift_norm)]\n",
    "                ch = out_ch\n",
    "                if resolution in cfg.attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, cfg.num_heads))\n",
    "                if level != len(cfg.channel_mult) - 1 and i == cfg.num_res_blocks:\n",
    "                    layers.append(Upsample(ch))\n",
    "                    resolution *= 2\n",
    "                self.ups.append(nn.ModuleList(layers))\n",
    "\n",
    "        self.final_norm = normalization(ch)\n",
    "        self.final_conv = nn.Conv2d(ch, cfg.out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_embed(t)\n",
    "        h = self.init_conv(x)\n",
    "        hs = [h]\n",
    "        for layers in self.downs:\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, ResidualBlock):\n",
    "                    h = layer(h, t_emb)\n",
    "                else:\n",
    "                    h = layer(h)\n",
    "            hs.append(h)\n",
    "        for layer in self.mid:\n",
    "            if isinstance(layer, ResidualBlock):\n",
    "                h = layer(h, t_emb)\n",
    "            else:\n",
    "                h = layer(h)\n",
    "        for layers in self.ups:\n",
    "            h = torch.cat([h, hs.pop()], dim=1)\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, ResidualBlock):\n",
    "                    h = layer(h, t_emb)\n",
    "                else:\n",
    "                    h = layer(h)\n",
    "        h = self.final_norm(h)\n",
    "        h = F.silu(h)\n",
    "        return self.final_conv(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753c987",
   "metadata": {
    "id": "e753c987"
   },
   "source": [
    "## 5. Diffusion + EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb10ea3",
   "metadata": {
    "id": "6cb10ea3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def cosine_beta_schedule(timesteps: int, s: float = 0.008) -> torch.Tensor:\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps) / timesteps\n",
    "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clamp(betas, 0.0001, 0.9999)\n",
    "\n",
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, model, cfg: DiffusionConfig):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.timesteps = cfg.timesteps\n",
    "        betas = cosine_beta_schedule(cfg.timesteps, cfg.s)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod - 1))\n",
    "        posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "        self.register_buffer('posterior_variance', posterior_variance)\n",
    "        self.register_buffer('posterior_log_variance_clipped', torch.log(torch.clamp(posterior_variance, min=1e-20)))\n",
    "        self.register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod))\n",
    "        self.register_buffer('posterior_mean_coef2', (1.0 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1.0 - alphas_cumprod))\n",
    "\n",
    "    def _extract(self, a, t, x_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        out = a.gather(-1, t)\n",
    "        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "    def q_sample(self, x_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_0.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape)\n",
    "        x_t = sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        return x_t, noise\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        sqrt_recip = self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape)\n",
    "        sqrt_recipm1 = self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        return sqrt_recip * x_t - sqrt_recipm1 * noise\n",
    "\n",
    "    def q_posterior(self, x_0, x_t, t):\n",
    "        mean = self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_0 + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        log_var = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return mean, log_var\n",
    "\n",
    "    def p_mean_variance(self, x_t, t):\n",
    "        predicted_noise = self.model(x_t, t)\n",
    "        x_0_pred = self.predict_start_from_noise(x_t, t, predicted_noise)\n",
    "        x_0_pred = torch.clamp(x_0_pred, -1.0, 1.0)\n",
    "        model_mean, model_log_variance = self.q_posterior(x_0_pred, x_t, t)\n",
    "        return model_mean, model_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x_t, t):\n",
    "        model_mean, model_log_variance = self.p_mean_variance(x_t, t)\n",
    "        noise = torch.randn_like(x_t)\n",
    "        nonzero_mask = (t != 0).float().reshape(-1, *((1,) * (len(x_t.shape) - 1)))\n",
    "        return model_mean + nonzero_mask * torch.exp(0.5 * model_log_variance) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size, image_size, channels, progress=True):\n",
    "        device = self.betas.device\n",
    "        x = torch.randn((batch_size, channels, image_size, image_size), device=device)\n",
    "        timesteps = list(reversed(range(self.timesteps)))\n",
    "        if progress:\n",
    "            timesteps = tqdm(timesteps, desc='Sampling', leave=False)\n",
    "        for t in timesteps:\n",
    "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            x = self.p_sample(x, t_batch)\n",
    "        return x\n",
    "\n",
    "    def compute_loss(self, x_0):\n",
    "        b = x_0.shape[0]\n",
    "        t = torch.randint(0, self.timesteps, (b,), device=x_0.device, dtype=torch.long)\n",
    "        noise = torch.randn_like(x_0)\n",
    "        x_t, _ = self.q_sample(x_0, t, noise)\n",
    "        predicted_noise = self.model(x_t, t)\n",
    "        return F.mse_loss(predicted_noise, noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306275d",
   "metadata": {
    "id": "d306275d"
   },
   "outputs": [],
   "source": [
    "\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "    def update(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            self.shadow[name] = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "    def apply_to(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in self.shadow:\n",
    "                param.data.copy_(self.shadow[name])\n",
    "    def state_dict(self):\n",
    "        return {'decay': self.decay, 'shadow': self.shadow}\n",
    "    def load_state_dict(self, state):\n",
    "        if not state:\n",
    "            return\n",
    "        self.decay = state.get('decay', self.decay)\n",
    "        self.shadow = state.get('shadow', self.shadow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eaac3d",
   "metadata": {
    "id": "78eaac3d"
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a105d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d9a105d4",
    "outputId": "c3b315fe-ca85-4838-fe53-75d2bef94bf1"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = UNet(config.model).to(DEVICE)\n",
    "diffusion = GaussianDiffusion(model, config.diffusion).to(DEVICE)\n",
    "ema = EMA(model, decay=config.training.ema_decay)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.training.learning_rate)\n",
    "scaler = GradScaler() if (config.training.mixed_precision and DEVICE.type == 'cuda') else None\n",
    "\n",
    "total_steps = max(1, config.training.total_steps)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps,\n",
    "    eta_min=2e-6,\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameters: {num_params:,}')\n",
    "\n",
    "OUTPUT_DIR = config.output_dir\n",
    "CHECKPOINT_DIR = Path(config.output_dir) / 'checkpoints' / MODEL_CONFIG_SIGNATURE\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Checkpoint directory: {CHECKPOINT_DIR}')\n",
    "\n",
    "def _summarize_keys(keys):\n",
    "    if not keys:\n",
    "        return 'none'\n",
    "    preview = ', '.join(keys[:5])\n",
    "    remaining = len(keys) - 5\n",
    "    if remaining > 0:\n",
    "        preview += f' (+{remaining} more)'\n",
    "    return preview\n",
    "\n",
    "def decode_latents(latents):\n",
    "    scale = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
    "    latents = latents / scale\n",
    "    latents = latents.to(dtype=vae.dtype)\n",
    "    images = vae.decode(latents).sample\n",
    "    images = (images + 1) / 2\n",
    "    return images.clamp(0, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def vae_sanity_check(n: int = 8, out_path: str = None):\n",
    "    # VAE-only reconstruction to verify normalization and scaling\n",
    "    loader = get_dataloader(\n",
    "        config.data.dataset,\n",
    "        config.data.image_size,\n",
    "        batch_size=n,\n",
    "        num_workers=config.training.num_workers,\n",
    "        train=False,\n",
    "    )\n",
    "    images, _ = next(iter(loader))\n",
    "    images = images.to(DEVICE)\n",
    "    if config.vae.use_fp16 and DEVICE.type == 'cuda':\n",
    "        images = images.half()\n",
    "\n",
    "    dist = vae.encode(images).latent_dist\n",
    "    latents = dist.sample()\n",
    "    if latents.shape[-1] != config.model.image_size:\n",
    "        print(\n",
    "            f'WARNING: latent size {latents.shape[-1]} != expected {config.model.image_size}'\n",
    "        )\n",
    "\n",
    "    scale = getattr(vae.config, 'scaling_factor', config.vae.latent_scaling_factor)\n",
    "    latents_scaled = latents * scale\n",
    "    recon = decode_latents(latents_scaled)\n",
    "\n",
    "    images_vis = (images + 1) / 2\n",
    "    images_vis = images_vis.clamp(0, 1)\n",
    "\n",
    "    grid = torch.cat([images_vis, recon], dim=0)\n",
    "    grid = torchvision.utils.make_grid(grid, nrow=n, padding=2)\n",
    "    if out_path is None:\n",
    "        out_path = str(Path(OUTPUT_DIR) / 'vae_recon.png')\n",
    "    torchvision.utils.save_image(grid, out_path)\n",
    "    print(f'Saved VAE sanity check: {out_path}')\n",
    "\n",
    "def log_samples(step):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        latents = diffusion.sample(64, config.model.image_size, config.model.out_channels, progress=False)\n",
    "        images = decode_latents(latents)\n",
    "    grid = torchvision.utils.make_grid(images, nrow=8, padding=2)\n",
    "    save_path = Path(OUTPUT_DIR) / f'samples_{step:08d}.png'\n",
    "    torchvision.utils.save_image(grid, save_path)\n",
    "    if wandb.run is not None:\n",
    "        wandb.log({'samples': wandb.Image(grid), 'step': step})\n",
    "    model.train()\n",
    "\n",
    "def _checkpoint_state(step, running_loss):\n",
    "    return {\n",
    "        'model': model.state_dict(),\n",
    "        'ema': ema.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'scaler': scaler.state_dict() if scaler is not None else None,\n",
    "        'step': step,\n",
    "        'running_loss': running_loss,\n",
    "        'model_config_signature': MODEL_CONFIG_SIGNATURE,\n",
    "        'model_config': MODEL_CONFIG_METADATA,\n",
    "    }\n",
    "\n",
    "def save_checkpoint(step, running_loss):\n",
    "    state = _checkpoint_state(step, running_loss)\n",
    "    ckpt_path = CHECKPOINT_DIR / f'ckpt_{step:08d}.pt'\n",
    "    torch.save(state, ckpt_path)\n",
    "    torch.save(state, CHECKPOINT_DIR / 'latest.pt')\n",
    "    print(f'Saved checkpoint: {ckpt_path}')\n",
    "\n",
    "def find_latest_checkpoint():\n",
    "    latest = CHECKPOINT_DIR / 'latest.pt'\n",
    "    if latest.exists():\n",
    "        return latest\n",
    "    candidates = sorted(CHECKPOINT_DIR.glob('ckpt_*.pt'))\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    state = torch.load(path, map_location=DEVICE)\n",
    "    ckpt_sig = state.get('model_config_signature')\n",
    "    if ckpt_sig is not None and ckpt_sig != MODEL_CONFIG_SIGNATURE:\n",
    "        print(\n",
    "            'Skipping checkpoint with mismatched model signature '\n",
    "            f\"(found {ckpt_sig}, expected {MODEL_CONFIG_SIGNATURE}).\"\n",
    "        )\n",
    "        return 0, 0.0\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    ckpt_keys = set(state['model'].keys())\n",
    "    missing = sorted(model_keys - ckpt_keys)\n",
    "    unexpected = sorted(ckpt_keys - model_keys)\n",
    "    if missing or unexpected:\n",
    "        print('Skipping checkpoint because parameter shapes changed:')\n",
    "        print(f'  Missing keys: {_summarize_keys(missing)}')\n",
    "        print(f'  Unexpected keys: {_summarize_keys(unexpected)}')\n",
    "        print('Delete the incompatible checkpoint directory if you intended to restart.')\n",
    "        return 0, 0.0\n",
    "    step = int(state.get('step', 0))\n",
    "    running_loss = float(state.get('running_loss', 0.0))\n",
    "    model.load_state_dict(state['model'])\n",
    "    ema.load_state_dict(state.get('ema', {}))\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    if state.get('scheduler') is not None:\n",
    "        scheduler.load_state_dict(state['scheduler'])\n",
    "    else:\n",
    "        scheduler.last_epoch = step - 1\n",
    "    if scaler is not None and state.get('scaler') is not None:\n",
    "        scaler.load_state_dict(state['scaler'])\n",
    "    print(f'Resumed from checkpoint: {path} (step={step})')\n",
    "    return step, running_loss\n",
    "\n",
    "WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
    "if WANDB_API_KEY:\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "wandb.init(project=f'ldm-{config.data.dataset}', name=config.exp_name, config={\n",
    "    'dataset': config.data.dataset,\n",
    "    'timesteps': config.diffusion.timesteps,\n",
    "    'beta_schedule': config.diffusion.beta_schedule,\n",
    "    'batch_size': config.training.batch_size,\n",
    "    'lr': config.training.learning_rate,\n",
    "    'image_size': config.data.image_size,\n",
    "    'latent_size': config.model.image_size,\n",
    "    'downsample_factor': config.vae.downsample_factor,\n",
    "    'model_params': num_params,\n",
    "    'model_signature': MODEL_CONFIG_SIGNATURE,\n",
    "})\n",
    "\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "latest_ckpt = find_latest_checkpoint()\n",
    "if latest_ckpt is not None:\n",
    "    step, running_loss = load_checkpoint(latest_ckpt)\n",
    "\n",
    "pbar = tqdm(total=config.training.total_steps, initial=step, desc='Training')\n",
    "\n",
    "while step < config.training.total_steps:\n",
    "    for batch in latent_loader:\n",
    "        if step >= config.training.total_steps:\n",
    "            break\n",
    "        latents = batch[0].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                loss = diffusion.compute_loss(latents)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            loss = diffusion.compute_loss(latents)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        ema.update(model)\n",
    "        step += 1\n",
    "        running_loss += loss.item()\n",
    "        if step % config.training.log_every == 0:\n",
    "            avg_loss = running_loss / config.training.log_every\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            wandb.log({'loss': avg_loss, 'lr': lr, 'step': step})\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.6f}'})\n",
    "            running_loss = 0.0\n",
    "        if step % config.training.sample_every == 0:\n",
    "            log_samples(step)\n",
    "        if step % config.training.save_every == 0:\n",
    "            save_checkpoint(step, running_loss)\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "save_checkpoint(step, running_loss)\n",
    "wandb.finish()\n",
    "print('Training done.')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
