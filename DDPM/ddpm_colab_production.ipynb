{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM (Denoising Diffusion Probabilistic Models) - TPU Production Training\n",
    "\n",
    "Google Colab TPU環境用の本番環境notebookです。\n",
    "\n",
    "**TPU最適化設定:**\n",
    "- Dataset: CIFAR-10\n",
    "- Total steps: 100,000\n",
    "- Batch size: 512 (TPU最適化)\n",
    "- Learning rate: 4e-4 (バッチサイズに合わせてスケール)\n",
    "- Timesteps: 1000\n",
    "- Runtime: TPU v2-8\n",
    "\n",
    "**Reference:**\n",
    "Ho et al. 2020 \"Denoising Diffusion Probabilistic Models\"\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Runtime > Change runtime type > TPU\n",
    "2. Run all cells in order\n",
    "3. Training will be faster than GPU due to TPU parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "TPU not available! Change runtime to TPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-858097709.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TPU確認\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/dev/accel0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TPU not available! Change runtime to TPU.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TPU is available!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: TPU not available! Change runtime to TPU."
     ]
    }
   ],
   "source": [
    "# TPU確認\n",
    "import os\n",
    "assert 'COLAB_TPU_ADDR' in os.environ or os.path.exists('/dev/accel0'), \"TPU not available! Change runtime to TPU.\"\n",
    "print(\"TPU is available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力ディレクトリを設定（ローカル）\n",
    "import os\n",
    "OUTPUT_DIR = './outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU用PyTorch/XLAのインストール\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q cloud-tpu-client https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
    "!pip install -q einops tqdm scipy pillow\n",
    "\n",
    "print(\"Package installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インポート\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Literal, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# PyTorch/XLA imports for TPU\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "# Device設定 (TPU)\n",
    "device = xm.xla_device()\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"TPU cores available: 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration (TPU Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"U-Net model configuration.\"\"\"\n",
    "    image_size: int = 32\n",
    "    in_channels: int = 3\n",
    "    out_channels: int = 3\n",
    "    model_channels: int = 128\n",
    "    channel_mult: tuple = (1, 2, 2, 2)\n",
    "    num_res_blocks: int = 2\n",
    "    attention_resolutions: tuple = (16,)\n",
    "    dropout: float = 0.0  # TPU: Disable dropout for faster training\n",
    "    num_heads: int = 4\n",
    "    use_scale_shift_norm: bool = True\n",
    "\n",
    "@dataclass\n",
    "class DiffusionConfig:\n",
    "    \"\"\"Diffusion process configuration.\"\"\"\n",
    "    timesteps: int = 1000\n",
    "    beta_schedule: Literal[\"linear\", \"cosine\", \"quadratic\"] = \"linear\"\n",
    "    beta_start: float = 1e-4\n",
    "    beta_end: float = 0.02\n",
    "    s: float = 0.008\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration optimized for TPU.\"\"\"\n",
    "    batch_size: int = 512  # TPU: Large batch size for efficiency\n",
    "    learning_rate: float = 4e-4  # TPU: Scaled for larger batch (linear scaling)\n",
    "    total_steps: int = 100_000\n",
    "    warmup_steps: int = 5000\n",
    "    grad_clip: float = 1.0\n",
    "    ema_decay: float = 0.9999\n",
    "    save_every: int = 5000\n",
    "    sample_every: int = 5000\n",
    "    log_every: int = 100\n",
    "    num_workers: int = 4  # TPU: More workers for data loading\n",
    "    mixed_precision: bool = False  # TPU: bfloat16 handled by XLA automatically\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Main configuration.\"\"\"\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    diffusion: DiffusionConfig = field(default_factory=DiffusionConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    output_dir: str = OUTPUT_DIR\n",
    "    data_dir: str = \"./data\"\n",
    "    device: str = \"xla\"  # TPU device\n",
    "    seed: int = 42\n",
    "    exp_name: str = \"ddpm_tpu_production\"\n",
    "\n",
    "# Create config\n",
    "config = Config()\n",
    "print(\"TPU Configuration:\")\n",
    "print(f\"  Total steps: {config.training.total_steps:,}\")\n",
    "print(f\"  Batch size: {config.training.batch_size} (TPU optimized)\")\n",
    "print(f\"  Learning rate: {config.training.learning_rate} (scaled for batch size)\")\n",
    "print(f\"  Timesteps: {config.diffusion.timesteps}\")\n",
    "print(f\"  Dropout: {config.model.dropout} (disabled for TPU speed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed設定\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    xm.set_rng_state(seed)\n",
    "\n",
    "set_seed(config.seed)\n",
    "print(f\"Random seed set to {config.seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"Sinusoidal position embeddings for timestep encoding.\"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time: torch.Tensor) -> torch.Tensor:\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"Swish activation function.\"\"\"\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class GroupNorm32(nn.GroupNorm):\n",
    "    \"\"\"GroupNorm with float32 computation for stability.\"\"\"\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "\n",
    "def normalization(channels: int, num_groups: int = 32) -> nn.Module:\n",
    "    \"\"\"Create a normalization layer.\"\"\"\n",
    "    return GroupNorm32(min(num_groups, channels), channels)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with timestep conditioning.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        time_emb_dim: int,\n",
    "        dropout: float = 0.0,\n",
    "        use_scale_shift_norm: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "\n",
    "        self.norm1 = normalization(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        self.norm2 = normalization(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "        # Time embedding projection\n",
    "        time_out_dim = out_channels * 2 if use_scale_shift_norm else out_channels\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            Swish(),\n",
    "            nn.Linear(time_emb_dim, time_out_dim),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Skip connection\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, time_emb: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.norm1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        # Add time embedding\n",
    "        time_emb = self.time_mlp(time_emb)\n",
    "        time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
    "\n",
    "        if self.use_scale_shift_norm:\n",
    "            scale, shift = time_emb.chunk(2, dim=1)\n",
    "            h = self.norm2(h) * (1 + scale) + shift\n",
    "        else:\n",
    "            h = h + time_emb\n",
    "            h = self.norm2(h)\n",
    "\n",
    "        h = F.silu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        return h + self.skip(x)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Multi-head self-attention block.\"\"\"\n",
    "    def __init__(self, channels: int, num_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = channels // num_heads\n",
    "\n",
    "        self.norm = normalization(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "\n",
    "        self.scale = self.head_dim**-0.5\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # Normalize\n",
    "        x_norm = self.norm(x)\n",
    "\n",
    "        # QKV projection\n",
    "        qkv = self.qkv(x_norm)\n",
    "        qkv = rearrange(qkv, \"b (three heads d) h w -> three b heads (h w) d\",\n",
    "                        three=3, heads=self.num_heads)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Attention\n",
    "        attn = torch.einsum(\"bhid,bhjd->bhij\", q, k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        # Aggregate\n",
    "        out = torch.einsum(\"bhij,bhjd->bhid\", attn, v)\n",
    "        out = rearrange(out, \"b heads (h w) d -> b (heads d) h w\", h=h, w=w)\n",
    "\n",
    "        # Project\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return x + out\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"Downsample by factor of 2.\"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"Upsample by factor of 2.\"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "print(\"Model utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net model for DDPM.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 32,\n",
    "        in_channels: int = 3,\n",
    "        out_channels: int = 3,\n",
    "        model_channels: int = 128,\n",
    "        channel_mult: tuple = (1, 2, 2, 2),\n",
    "        num_res_blocks: int = 2,\n",
    "        attention_resolutions: tuple = (16,),\n",
    "        dropout: float = 0.0,\n",
    "        num_heads: int = 4,\n",
    "        use_scale_shift_norm: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "\n",
    "        # Time embedding\n",
    "        time_emb_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(model_channels),\n",
    "            nn.Linear(model_channels, time_emb_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "\n",
    "        # Initial projection\n",
    "        self.init_conv = nn.Conv2d(in_channels, model_channels, 3, padding=1)\n",
    "\n",
    "        # Build encoder\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "\n",
    "        channels = [model_channels]\n",
    "        ch = model_channels\n",
    "        resolution = image_size\n",
    "\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            out_ch = model_channels * mult\n",
    "\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResidualBlock(\n",
    "                        ch, out_ch, time_emb_dim, dropout, use_scale_shift_norm\n",
    "                    )\n",
    "                ]\n",
    "                ch = out_ch\n",
    "\n",
    "                if resolution in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads))\n",
    "\n",
    "                self.downs.append(nn.ModuleList(layers))\n",
    "                channels.append(ch)\n",
    "\n",
    "            # Downsample (except last level)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                self.downs.append(nn.ModuleList([Downsample(ch)]))\n",
    "                channels.append(ch)\n",
    "                resolution //= 2\n",
    "\n",
    "        # Middle block\n",
    "        self.mid = nn.ModuleList([\n",
    "            ResidualBlock(ch, ch, time_emb_dim, dropout, use_scale_shift_norm),\n",
    "            AttentionBlock(ch, num_heads),\n",
    "            ResidualBlock(ch, ch, time_emb_dim, dropout, use_scale_shift_norm),\n",
    "        ])\n",
    "\n",
    "        # Build decoder\n",
    "        for level, mult in enumerate(reversed(channel_mult)):\n",
    "            out_ch = model_channels * mult\n",
    "\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                skip_ch = channels.pop()\n",
    "                layers = [\n",
    "                    ResidualBlock(\n",
    "                        ch + skip_ch, out_ch, time_emb_dim, dropout, use_scale_shift_norm\n",
    "                    )\n",
    "                ]\n",
    "                ch = out_ch\n",
    "\n",
    "                if resolution in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads))\n",
    "\n",
    "                # Upsample (except last block of each level, and not on last level)\n",
    "                if level != len(channel_mult) - 1 and i == num_res_blocks:\n",
    "                    layers.append(Upsample(ch))\n",
    "                    resolution *= 2\n",
    "\n",
    "                self.ups.append(nn.ModuleList(layers))\n",
    "\n",
    "        # Final projection\n",
    "        self.final_norm = normalization(ch)\n",
    "        self.final_conv = nn.Conv2d(ch, out_channels, 3, padding=1)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with proper scaling.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "        # Zero initialize the final conv for better training stability\n",
    "        nn.init.zeros_(self.final_conv.weight)\n",
    "        nn.init.zeros_(self.final_conv.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input images [B, C, H, W]\n",
    "            t: Timesteps [B]\n",
    "\n",
    "        Returns:\n",
    "            Predicted noise [B, C, H, W]\n",
    "        \"\"\"\n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t)\n",
    "\n",
    "        # Initial conv\n",
    "        h = self.init_conv(x)\n",
    "        hs = [h]\n",
    "\n",
    "        # Encoder\n",
    "        for layers in self.downs:\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, ResidualBlock):\n",
    "                    h = layer(h, t_emb)\n",
    "                elif isinstance(layer, AttentionBlock):\n",
    "                    h = layer(h)\n",
    "                else:  # Downsample\n",
    "                    h = layer(h)\n",
    "            hs.append(h)\n",
    "\n",
    "        # Middle\n",
    "        for layer in self.mid:\n",
    "            if isinstance(layer, ResidualBlock):\n",
    "                h = layer(h, t_emb)\n",
    "            else:\n",
    "                h = layer(h)\n",
    "\n",
    "        # Decoder\n",
    "        for layers in self.ups:\n",
    "            h = torch.cat([h, hs.pop()], dim=1)\n",
    "            for layer in layers:\n",
    "                if isinstance(layer, ResidualBlock):\n",
    "                    h = layer(h, t_emb)\n",
    "                elif isinstance(layer, AttentionBlock):\n",
    "                    h = layer(h)\n",
    "                else:  # Upsample\n",
    "                    h = layer(h)\n",
    "\n",
    "        # Final\n",
    "        h = self.final_norm(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.final_conv(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "def create_model(cfg: ModelConfig) -> UNet:\n",
    "    \"\"\"Create U-Net model from config.\"\"\"\n",
    "    return UNet(\n",
    "        image_size=cfg.image_size,\n",
    "        in_channels=cfg.in_channels,\n",
    "        out_channels=cfg.out_channels,\n",
    "        model_channels=cfg.model_channels,\n",
    "        channel_mult=cfg.channel_mult,\n",
    "        num_res_blocks=cfg.num_res_blocks,\n",
    "        attention_resolutions=cfg.attention_resolutions,\n",
    "        dropout=cfg.dropout,\n",
    "        num_heads=cfg.num_heads,\n",
    "        use_scale_shift_norm=cfg.use_scale_shift_norm,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"UNet model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta_schedule(schedule_type: str, timesteps: int, beta_start: float = 1e-4, \n",
    "                      beta_end: float = 0.02, s: float = 0.008):\n",
    "    \"\"\"\n",
    "    Get beta schedule for diffusion process.\n",
    "    \"\"\"\n",
    "    if schedule_type == \"linear\":\n",
    "        return torch.linspace(beta_start, beta_end, timesteps)\n",
    "    elif schedule_type == \"quadratic\":\n",
    "        return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
    "    elif schedule_type == \"cosine\":\n",
    "        steps = timesteps + 1\n",
    "        t = torch.linspace(0, timesteps, steps)\n",
    "        alphas_cumprod = torch.cos(((t / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        return torch.clip(betas, 0.0001, 0.9999)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule type: {schedule_type}\")\n",
    "\n",
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, model: nn.Module, config: DiffusionConfig):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        \n",
    "        # Beta schedule\n",
    "        betas = get_beta_schedule(\n",
    "            config.beta_schedule, \n",
    "            config.timesteps, \n",
    "            config.beta_start, \n",
    "            config.beta_end,\n",
    "            config.s\n",
    "        )\n",
    "        \n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # Register buffers\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        \n",
    "        # Calculations for diffusion q(x_t | x_{t-1})\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - alphas_cumprod))\n",
    "        \n",
    "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "        self.register_buffer('posterior_variance', posterior_variance)\n",
    "        self.register_buffer('posterior_log_variance_clipped', \n",
    "                             torch.log(torch.clamp(posterior_variance, min=1e-20)))\n",
    "        self.register_buffer('posterior_mean_coef1', \n",
    "                             betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod))\n",
    "        self.register_buffer('posterior_mean_coef2', \n",
    "                             (1.0 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1.0 - alphas_cumprod))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data (forward process).\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t][:, None, None, None]\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
    "        \n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_mean_variance(self, x, t):\n",
    "        \"\"\"\n",
    "        Predict mean and variance for reverse process.\n",
    "        \"\"\"\n",
    "        # Predict noise\n",
    "        pred_noise = self.model(x, t)\n",
    "        \n",
    "        # Get coefficients\n",
    "        sqrt_recip_alphas_cumprod = 1.0 / self.sqrt_alphas_cumprod\n",
    "        sqrt_recipm1_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod / self.sqrt_alphas_cumprod\n",
    "        \n",
    "        # Predict x_0\n",
    "        sqrt_recip_alphas_cumprod_t = sqrt_recip_alphas_cumprod[t][:, None, None, None]\n",
    "        sqrt_recipm1_alphas_cumprod_t = sqrt_recipm1_alphas_cumprod[t][:, None, None, None]\n",
    "        \n",
    "        pred_x0 = sqrt_recip_alphas_cumprod_t * x - sqrt_recipm1_alphas_cumprod_t * pred_noise\n",
    "        pred_x0 = torch.clamp(pred_x0, -1, 1)\n",
    "        \n",
    "        # Get posterior mean and variance\n",
    "        posterior_mean_coef1_t = self.posterior_mean_coef1[t][:, None, None, None]\n",
    "        posterior_mean_coef2_t = self.posterior_mean_coef2[t][:, None, None, None]\n",
    "        \n",
    "        posterior_mean = posterior_mean_coef1_t * pred_x0 + posterior_mean_coef2_t * x\n",
    "        posterior_variance = self.posterior_variance[t][:, None, None, None]\n",
    "        \n",
    "        return posterior_mean, posterior_variance\n",
    "\n",
    "    def p_sample(self, x, t):\n",
    "        \"\"\"\n",
    "        Sample from reverse process.\n",
    "        \"\"\"\n",
    "        mean, variance = self.p_mean_variance(x, t)\n",
    "        noise = torch.randn_like(x)\n",
    "        # No noise when t == 0\n",
    "        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        return mean + nonzero_mask * torch.sqrt(variance) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size: int, image_size: int, device, progress: bool = True):\n",
    "        \"\"\"\n",
    "        Generate samples from the model.\n",
    "        \"\"\"\n",
    "        # Start from pure noise\n",
    "        x = torch.randn(batch_size, 3, image_size, image_size, device=device)\n",
    "        \n",
    "        # Denoise progressively\n",
    "        timesteps = list(range(self.config.timesteps))[::-1]\n",
    "        if progress:\n",
    "            timesteps = tqdm(timesteps, desc=\"Sampling\")\n",
    "        \n",
    "        for t in timesteps:\n",
    "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            x = self.p_sample(x, t_batch)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x_start):\n",
    "        \"\"\"\n",
    "        Training loss (simplified MSE loss).\n",
    "        \"\"\"\n",
    "        batch_size = x_start.shape[0]\n",
    "        device = x_start.device\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, self.config.timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(x_start)\n",
    "        \n",
    "        # Add noise to images\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        \n",
    "        # Predict noise\n",
    "        pred_noise = self.model(x_noisy, t)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "print(\"Diffusion process defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. EMA (Exponential Moving Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    \"\"\"\n",
    "    Exponential Moving Average of model parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.original = {}\n",
    "        \n",
    "        # Initialize shadow parameters\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self, model):\n",
    "        \"\"\"Update EMA parameters.\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] -= (1 - self.decay) * (self.shadow[name] - param.data)\n",
    "\n",
    "    def apply_shadow(self, model):\n",
    "        \"\"\"Apply EMA parameters to model.\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.original[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self, model):\n",
    "        \"\"\"Restore original parameters.\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.original[name]\n",
    "        self.original = {}\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'shadow': self.shadow, 'decay': self.decay}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.shadow = state_dict['shadow']\n",
    "        self.decay = state_dict['decay']\n",
    "\n",
    "print(\"EMA class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Loading (TPU Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Scale to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_dataset = CIFAR10(\n",
    "    root=config.data_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# TPU: Use regular DataLoader, ParallelLoader will wrap it\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.training.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.training.num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset: CIFAR-10\")\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Batch size: {config.training.batch_size} (TPU optimized)\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training (TPU Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and move to TPU\n",
    "model = create_model(config.model)\n",
    "diffusion = GaussianDiffusion(model, config.diffusion)\n",
    "\n",
    "# Move to TPU\n",
    "model = model.to(device)\n",
    "diffusion = diffusion.to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "\n",
    "# EMA\n",
    "ema = EMA(model, decay=config.training.ema_decay)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.training.learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.0\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "def lr_lambda(step):\n",
    "    if step < config.training.warmup_steps:\n",
    "        return step / config.training.warmup_steps\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "print(\"Model initialized on TPU and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization utilities\n",
    "def unnormalize(x):\n",
    "    \"\"\"Unnormalize images from [-1, 1] to [0, 1].\"\"\"\n",
    "    return (x + 1) / 2\n",
    "\n",
    "def plot_samples(samples, title=\"Generated Samples\"):\n",
    "    \"\"\"Plot a grid of samples.\"\"\"\n",
    "    samples = unnormalize(samples)\n",
    "    samples = torch.clamp(samples, 0, 1)\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(samples, nrow=8, padding=2)\n",
    "    grid_np = grid.cpu().permute(1, 2, 0).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(grid_np)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_checkpoint(step, model, optimizer, ema, scheduler, metrics, path):\n",
    "    \"\"\"Save training checkpoint (TPU compatible).\"\"\"\n",
    "    # Move model to CPU for saving\n",
    "    cpu_model_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "    cpu_ema_shadow = {k: v.cpu() for k, v in ema.shadow.items()}\n",
    "    \n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model': cpu_model_state,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'ema': {'shadow': cpu_ema_shadow, 'decay': ema.decay},\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'metrics': metrics,\n",
    "    }\n",
    "    xm.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved: {path}\")\n",
    "\n",
    "print(\"Utilities ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (TPU optimized)\n",
    "print(\"Starting TPU training...\")\n",
    "print(f\"Total steps: {config.training.total_steps:,}\")\n",
    "print(f\"Batch size: {config.training.batch_size} (TPU optimized)\")\n",
    "print(f\"Learning rate: {config.training.learning_rate}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Setup directories\n",
    "checkpoint_dir = Path(config.output_dir) / \"checkpoints\"\n",
    "sample_dir = Path(config.output_dir) / \"samples\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training state\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "losses = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=config.training.total_steps, desc=\"Training (TPU)\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "epoch = 0\n",
    "\n",
    "while step < config.training.total_steps:\n",
    "    epoch += 1\n",
    "    \n",
    "    # TPU: Wrap DataLoader with ParallelLoader for efficient data transfer\n",
    "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
    "    \n",
    "    for batch in para_loader.per_device_loader(device):\n",
    "        if step >= config.training.total_steps:\n",
    "            break\n",
    "        \n",
    "        images = batch[0]  # Already on TPU device\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss = diffusion(images)\n",
    "        loss.backward()\n",
    "        \n",
    "        # TPU: Clip gradients\n",
    "        xm.reduce_gradients(optimizer)  # Sync gradients across TPU cores\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)\n",
    "        \n",
    "        # TPU: Optimizer step with barrier\n",
    "        xm.optimizer_step(optimizer)\n",
    "        \n",
    "        scheduler.step()\n",
    "        ema.update(model)\n",
    "        \n",
    "        step += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Logging\n",
    "        if step % config.training.log_every == 0:\n",
    "            avg_loss = running_loss / config.training.log_every\n",
    "            losses.append(avg_loss)\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            elapsed = (datetime.now() - start_time).total_seconds()\n",
    "            steps_per_sec = step / elapsed\n",
    "            remaining_steps = config.training.total_steps - step\n",
    "            eta_seconds = remaining_steps / steps_per_sec\n",
    "            eta_hours = eta_seconds / 3600\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'lr': f'{lr:.6f}',\n",
    "                'ETA': f'{eta_hours:.1f}h',\n",
    "                'step/s': f'{steps_per_sec:.2f}'\n",
    "            })\n",
    "            \n",
    "            running_loss = 0.0\n",
    "        \n",
    "        # Generate samples\n",
    "        if step % config.training.sample_every == 0 or step == config.training.total_steps:\n",
    "            print(f\"\\n[Step {step}/{config.training.total_steps}] Generating samples...\")\n",
    "            \n",
    "            # Use EMA model for sampling\n",
    "            ema.apply_shadow(model)\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                samples = diffusion.sample(\n",
    "                    batch_size=64,\n",
    "                    image_size=config.model.image_size,\n",
    "                    device=device,\n",
    "                    progress=False\n",
    "                )\n",
    "            \n",
    "            # Move samples to CPU for saving\n",
    "            samples_cpu = samples.cpu()\n",
    "            \n",
    "            # Save samples\n",
    "            samples_unnorm = unnormalize(samples_cpu)\n",
    "            samples_unnorm = torch.clamp(samples_unnorm, 0, 1)\n",
    "            grid = torchvision.utils.make_grid(samples_unnorm, nrow=8, padding=2)\n",
    "            save_path = sample_dir / f\"samples_step_{step:08d}.png\"\n",
    "            torchvision.utils.save_image(grid, save_path)\n",
    "            \n",
    "            # Display samples\n",
    "            clear_output(wait=True)\n",
    "            plot_samples(samples_cpu, title=f\"Samples at step {step}\")\n",
    "            \n",
    "            ema.restore(model)\n",
    "            model.train()\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if step % config.training.save_every == 0 or step == config.training.total_steps:\n",
    "            checkpoint_path = checkpoint_dir / f\"checkpoint_step_{step:08d}.pt\"\n",
    "            save_checkpoint(\n",
    "                step=step,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                ema=ema,\n",
    "                scheduler=scheduler,\n",
    "                metrics={'loss': loss.item()},\n",
    "                path=str(checkpoint_path)\n",
    "            )\n",
    "            \n",
    "            # Also save as latest\n",
    "            latest_path = checkpoint_dir / \"checkpoint_latest.pt\"\n",
    "            save_checkpoint(\n",
    "                step=step,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                ema=ema,\n",
    "                scheduler=scheduler,\n",
    "                metrics={'loss': loss.item()},\n",
    "                path=str(latest_path)\n",
    "            )\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Training complete\n",
    "total_time = (datetime.now() - start_time).total_seconds() / 3600\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Total time: {total_time:.2f} hours\")\n",
    "print(f\"Final loss: {loss.item():.4f}\")\n",
    "print(f\"Checkpoints saved to: {checkpoint_dir}\")\n",
    "print(f\"Samples saved to: {sample_dir}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step (x100)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(config.output_dir) / 'training_loss.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final samples\n",
    "print(\"Generating final samples with EMA model...\")\n",
    "\n",
    "ema.apply_shadow(model)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    final_samples = diffusion.sample(\n",
    "        batch_size=64,\n",
    "        image_size=config.model.image_size,\n",
    "        device=device,\n",
    "        progress=True\n",
    "    )\n",
    "\n",
    "final_samples_cpu = final_samples.cpu()\n",
    "plot_samples(final_samples_cpu, title=\"Final Generated Samples\")\n",
    "\n",
    "# Save final samples\n",
    "final_samples_unnorm = unnormalize(final_samples_cpu)\n",
    "final_samples_unnorm = torch.clamp(final_samples_unnorm, 0, 1)\n",
    "grid = torchvision.utils.make_grid(final_samples_unnorm, nrow=8, padding=2)\n",
    "torchvision.utils.save_image(grid, Path(config.output_dir) / 'final_samples.png')\n",
    "\n",
    "ema.restore(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training summary\n",
    "summary = {\n",
    "    'exp_name': config.exp_name,\n",
    "    'total_steps': step,\n",
    "    'total_hours': total_time,\n",
    "    'timesteps': config.diffusion.timesteps,\n",
    "    'beta_schedule': config.diffusion.beta_schedule,\n",
    "    'batch_size': config.training.batch_size,\n",
    "    'learning_rate': config.training.learning_rate,\n",
    "    'final_loss': loss.item(),\n",
    "    'num_parameters': num_params,\n",
    "    'device': 'TPU',\n",
    "}\n",
    "\n",
    "summary_path = Path(config.output_dir) / 'training_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference - Generate More Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional samples\n",
    "print(\"Generating additional samples...\")\n",
    "\n",
    "ema.apply_shadow(model)\n",
    "model.eval()\n",
    "\n",
    "num_batches = 4  # Generate 4 batches of 64 samples each\n",
    "\n",
    "for i in range(num_batches):\n",
    "    with torch.no_grad():\n",
    "        samples = diffusion.sample(\n",
    "            batch_size=64,\n",
    "            image_size=config.model.image_size,\n",
    "            device=device,\n",
    "            progress=True\n",
    "        )\n",
    "    \n",
    "    samples_cpu = samples.cpu()\n",
    "    plot_samples(samples_cpu, title=f\"Generated Samples Batch {i+1}\")\n",
    "\n",
    "ema.restore(model)\n",
    "\n",
    "print(f\"Generated {num_batches * 64} total samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### TPU Performance Expectations\n",
    "\n",
    "- **Training speed**: TPUはGPU (T4)より大幅に高速\n",
    "- **Batch size**: 512 (TPU最適化)\n",
    "- **Learning rate**: 4e-4 (バッチサイズに合わせてスケール)\n",
    "\n",
    "### TPU Optimization Tips\n",
    "\n",
    "1. **Dropout**: 0に設定（TPUでは高速化のため）\n",
    "2. **Batch size**: 大きいほど効率的（512推奨）\n",
    "3. **ParallelLoader**: データ転送を最適化\n",
    "4. **xm.optimizer_step()**: TPU用の勾配同期\n",
    "\n",
    "### Resume Training\n",
    "\n",
    "接続が切れた場合、以下のコードで再開:\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load('./outputs/checkpoints/checkpoint_latest.pt')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "ema.load_state_dict(checkpoint['ema'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "step = checkpoint['step']\n",
    "# Move to TPU\n",
    "model = model.to(device)\n",
    "```\n",
    "\n",
    "### References\n",
    "\n",
    "- Paper: [Denoising Diffusion Probabilistic Models (Ho et al., 2020)](https://arxiv.org/abs/2006.11239)\n",
    "- Original results: FID 3.17 on CIFAR-10 (800,000 steps)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
