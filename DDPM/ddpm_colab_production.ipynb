{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# DDPM (Denoising Diffusion Probabilistic Models) - Production Training\n",
    "\n",
    "Google Colab無料版（12時間制限）で完了する本番環境用のnotebookです。\n",
    "\n",
    "**設定:**\n",
    "- Dataset: CIFAR-10\n",
    "- Total steps: 100,000 (約10-11時間)\n",
    "- Batch size: 128\n",
    "- Timesteps: 1000\n",
    "- GPU: T4 (Colab free tier)\n",
    "\n",
    "**Reference:**\n",
    "Ho et al. 2020 \"Denoising Diffusion Probabilistic Models\"\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Runtime > Change runtime type > GPU (T4)\n",
    "2. Run all cells in order\n",
    "3. Training will take approximately 10-11 hours\n",
    "4. Checkpoints are saved to Google Drive (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# GPU確認\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Google Driveをマウント（チェックポイント保存用）\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 出力ディレクトリをDriveに設定\n",
    "import os\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/DDPM_outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# 必要なパッケージのインストール\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q einops tqdm scipy pillow\n",
    "\n",
    "print(\"Package installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# インポート\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Literal, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Device設定\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_classes"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"U-Net model configuration.\"\"\"\n",
    "    image_size: int = 32\n",
    "    in_channels: int = 3\n",
    "    out_channels: int = 3\n",
    "    model_channels: int = 128\n",
    "    channel_mult: tuple = (1, 2, 2, 2)\n",
    "    num_res_blocks: int = 2\n",
    "    attention_resolutions: tuple = (16,)\n",
    "    dropout: float = 0.1\n",
    "    num_heads: int = 4\n",
    "    use_scale_shift_norm: bool = True\n",
    "\n",
    "@dataclass\n",
    "class DiffusionConfig:\n",
    "    \"\"\"Diffusion process configuration.\"\"\"\n",
    "    timesteps: int = 1000\n",
    "    beta_schedule: Literal[\"linear\", \"cosine\", \"quadratic\"] = \"linear\"\n",
    "    beta_start: float = 1e-4\n",
    "    beta_end: float = 0.02\n",
    "    s: float = 0.008\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration for Colab free tier (12 hours).\"\"\"\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 2e-4\n",
    "    total_steps: int = 100_000  # ~10-11 hours on T4\n",
    "    warmup_steps: int = 5000\n",
    "    grad_clip: float = 1.0\n",
    "    ema_decay: float = 0.9999\n",
    "    save_every: int = 5000\n",
    "    sample_every: int = 5000\n",
    "    log_every: int = 100\n",
    "    num_workers: int = 2\n",
    "    mixed_precision: bool = True\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Main configuration.\"\"\"\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    diffusion: DiffusionConfig = field(default_factory=DiffusionConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    output_dir: str = OUTPUT_DIR\n",
    "    data_dir: str = \"./data\"\n",
    "    device: str = \"cuda\"\n",
    "    seed: int = 42\n",
    "    exp_name: str = \"ddpm_colab_production\"\n",
    "\n",
    "# Create config\n",
    "config = Config()\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Total steps: {config.training.total_steps:,}\")\n",
    "print(f\"  Batch size: {config.training.batch_size}\")\n",
    "print(f\"  Timesteps: {config.diffusion.timesteps}\")\n",
    "print(f\"  Estimated time: ~10-11 hours on T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seed"
   },
   "outputs": [],
   "source": [
    "# Seed設定\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(config.seed)\n",
    "print(f\"Random seed set to {config.seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model"
   },
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_utils"
   },
   "outputs": [],
   "source": [
    "# Utility functions for sinusoidal position embeddings\n",
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    Sinusoidal timestep embeddings.\n",
    "    \"\"\"\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:\n",
    "        emb = F.pad(emb, (0, 1, 0, 0))\n",
    "    return emb\n",
    "\n",
    "class TimestepEmbedding(nn.Module):\n",
    "    def __init__(self, dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dim, out_dim)\n",
    "        self.act = nn.SiLU()\n",
    "        self.linear2 = nn.Linear(out_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "def normalization(channels):\n",
    "    return nn.GroupNorm(32, channels)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_channels, dropout, use_scale_shift_norm=True):\n",
    "        super().__init__()\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "        \n",
    "        self.norm1 = normalization(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        \n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_channels, out_channels * 2 if use_scale_shift_norm else out_channels),\n",
    "        )\n",
    "        \n",
    "        self.norm2 = normalization(out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        h = self.norm1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        t_emb = self.time_emb(t)[:, :, None, None]\n",
    "        \n",
    "        if self.use_scale_shift_norm:\n",
    "            scale, shift = t_emb.chunk(2, dim=1)\n",
    "            h = self.norm2(h) * (1 + scale) + shift\n",
    "        else:\n",
    "            h = h + t_emb\n",
    "            h = self.norm2(h)\n",
    "        \n",
    "        h = F.silu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "        \n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.norm = normalization(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        qkv = rearrange(qkv, 'b (three heads c) h w -> three b heads c (h w)', three=3, heads=self.num_heads)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = torch.einsum('bhci,bhcj->bhij', q, k) * (c ** -0.5)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = torch.einsum('bhij,bhcj->bhci', attn, v)\n",
    "        out = rearrange(out, 'b heads c (h w) -> b (heads c) h w', h=h, w=w)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return x + out\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        return self.conv(x)\n",
    "\n",
    "print(\"Model utilities defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unet"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Time embedding\n",
    "        time_embed_dim = config.model_channels * 4\n",
    "        self.time_embed = TimestepEmbedding(config.model_channels, time_embed_dim)\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv_in = nn.Conv2d(config.in_channels, config.model_channels, 3, padding=1)\n",
    "        \n",
    "        # Downsampling\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        channels = [config.model_channels]\n",
    "        ch = config.model_channels\n",
    "        \n",
    "        for level, mult in enumerate(config.channel_mult):\n",
    "            out_ch = config.model_channels * mult\n",
    "            for _ in range(config.num_res_blocks):\n",
    "                layers = [ResBlock(ch, out_ch, time_embed_dim, config.dropout, config.use_scale_shift_norm)]\n",
    "                ch = out_ch\n",
    "                if config.image_size // (2 ** level) in config.attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, config.num_heads))\n",
    "                self.down_blocks.append(nn.ModuleList(layers))\n",
    "                channels.append(ch)\n",
    "            \n",
    "            if level != len(config.channel_mult) - 1:\n",
    "                self.down_blocks.append(nn.ModuleList([Downsample(ch)]))\n",
    "                channels.append(ch)\n",
    "        \n",
    "        # Middle\n",
    "        self.middle = nn.ModuleList([\n",
    "            ResBlock(ch, ch, time_embed_dim, config.dropout, config.use_scale_shift_norm),\n",
    "            AttentionBlock(ch, config.num_heads),\n",
    "            ResBlock(ch, ch, time_embed_dim, config.dropout, config.use_scale_shift_norm),\n",
    "        ])\n",
    "        \n",
    "        # Upsampling\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for level, mult in enumerate(reversed(config.channel_mult)):\n",
    "            for i in range(config.num_res_blocks + 1):\n",
    "                skip_ch = channels.pop()\n",
    "                layers = [ResBlock(ch + skip_ch, config.model_channels * mult, time_embed_dim, \n",
    "                                   config.dropout, config.use_scale_shift_norm)]\n",
    "                ch = config.model_channels * mult\n",
    "                if config.image_size // (2 ** (len(config.channel_mult) - 1 - level)) in config.attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, config.num_heads))\n",
    "                if level < len(config.channel_mult) - 1 and i == config.num_res_blocks:\n",
    "                    layers.append(Upsample(ch))\n",
    "                self.up_blocks.append(nn.ModuleList(layers))\n",
    "        \n",
    "        # Output\n",
    "        self.norm_out = normalization(ch)\n",
    "        self.conv_out = nn.Conv2d(ch, config.out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Time embedding\n",
    "        t_emb = get_timestep_embedding(t, self.config.model_channels)\n",
    "        t_emb = self.time_embed(t_emb)\n",
    "        \n",
    "        # Initial conv\n",
    "        h = self.conv_in(x)\n",
    "        hs = [h]\n",
    "        \n",
    "        # Downsampling\n",
    "        for blocks in self.down_blocks:\n",
    "            for block in blocks:\n",
    "                if isinstance(block, ResBlock):\n",
    "                    h = block(h, t_emb)\n",
    "                elif isinstance(block, AttentionBlock):\n",
    "                    h = block(h)\n",
    "                else:  # Downsample\n",
    "                    h = block(h)\n",
    "                hs.append(h)\n",
    "        \n",
    "        # Middle\n",
    "        for block in self.middle:\n",
    "            if isinstance(block, ResBlock):\n",
    "                h = block(h, t_emb)\n",
    "            else:\n",
    "                h = block(h)\n",
    "        \n",
    "        # Upsampling\n",
    "        for blocks in self.up_blocks:\n",
    "            skip = hs.pop()\n",
    "            h = torch.cat([h, skip], dim=1)\n",
    "            for block in blocks:\n",
    "                if isinstance(block, ResBlock):\n",
    "                    h = block(h, t_emb)\n",
    "                elif isinstance(block, AttentionBlock):\n",
    "                    h = block(h)\n",
    "                else:  # Upsample\n",
    "                    h = block(h)\n",
    "        \n",
    "        # Output\n",
    "        h = self.norm_out(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv_out(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "print(\"UNet model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diffusion"
   },
   "source": [
    "## 4. Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diffusion_class"
   },
   "outputs": [],
   "source": [
    "def get_beta_schedule(schedule_type: str, timesteps: int, beta_start: float = 1e-4, \n",
    "                      beta_end: float = 0.02, s: float = 0.008):\n",
    "    \"\"\"\n",
    "    Get beta schedule for diffusion process.\n",
    "    \"\"\"\n",
    "    if schedule_type == \"linear\":\n",
    "        return torch.linspace(beta_start, beta_end, timesteps)\n",
    "    elif schedule_type == \"quadratic\":\n",
    "        return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
    "    elif schedule_type == \"cosine\":\n",
    "        steps = timesteps + 1\n",
    "        t = torch.linspace(0, timesteps, steps)\n",
    "        alphas_cumprod = torch.cos(((t / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        return torch.clip(betas, 0.0001, 0.9999)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule type: {schedule_type}\")\n",
    "\n",
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, model: nn.Module, config: DiffusionConfig):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        \n",
    "        # Beta schedule\n",
    "        betas = get_beta_schedule(\n",
    "            config.beta_schedule, \n",
    "            config.timesteps, \n",
    "            config.beta_start, \n",
    "            config.beta_end,\n",
    "            config.s\n",
    "        )\n",
    "        \n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # Register buffers\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "        \n",
    "        # Calculations for diffusion q(x_t | x_{t-1})\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - alphas_cumprod))\n",
    "        \n",
    "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "        self.register_buffer('posterior_variance', posterior_variance)\n",
    "        self.register_buffer('posterior_log_variance_clipped', \n",
    "                             torch.log(torch.clamp(posterior_variance, min=1e-20)))\n",
    "        self.register_buffer('posterior_mean_coef1', \n",
    "                             betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod))\n",
    "        self.register_buffer('posterior_mean_coef2', \n",
    "                             (1.0 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1.0 - alphas_cumprod))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data (forward process).\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t][:, None, None, None]\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
    "        \n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_mean_variance(self, x, t):\n",
    "        \"\"\"\n",
    "        Predict mean and variance for reverse process.\n",
    "        \"\"\"\n",
    "        # Predict noise\n",
    "        pred_noise = self.model(x, t)\n",
    "        \n",
    "        # Get coefficients\n",
    "        sqrt_recip_alphas_cumprod = 1.0 / self.sqrt_alphas_cumprod\n",
    "        sqrt_recipm1_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod / self.sqrt_alphas_cumprod\n",
    "        \n",
    "        # Predict x_0\n",
    "        sqrt_recip_alphas_cumprod_t = sqrt_recip_alphas_cumprod[t][:, None, None, None]\n",
    "        sqrt_recipm1_alphas_cumprod_t = sqrt_recipm1_alphas_cumprod[t][:, None, None, None]\n",
    "        \n",
    "        pred_x0 = sqrt_recip_alphas_cumprod_t * x - sqrt_recipm1_alphas_cumprod_t * pred_noise\n",
    "        pred_x0 = torch.clamp(pred_x0, -1, 1)\n",
    "        \n",
    "        # Get posterior mean and variance\n",
    "        posterior_mean_coef1_t = self.posterior_mean_coef1[t][:, None, None, None]\n",
    "        posterior_mean_coef2_t = self.posterior_mean_coef2[t][:, None, None, None]\n",
    "        \n",
    "        posterior_mean = posterior_mean_coef1_t * pred_x0 + posterior_mean_coef2_t * x\n",
    "        posterior_variance = self.posterior_variance[t][:, None, None, None]\n",
    "        \n",
    "        return posterior_mean, posterior_variance\n",
    "\n",
    "    def p_sample(self, x, t):\n",
    "        \"\"\"\n",
    "        Sample from reverse process.\n",
    "        \"\"\"\n",
    "        mean, variance = self.p_mean_variance(x, t)\n",
    "        noise = torch.randn_like(x)\n",
    "        # No noise when t == 0\n",
    "        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        return mean + nonzero_mask * torch.sqrt(variance) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size: int, image_size: int, progress: bool = True):\n",
    "        \"\"\"\n",
    "        Generate samples from the model.\n",
    "        \"\"\"\n",
    "        device = next(self.model.parameters()).device\n",
    "        \n",
    "        # Start from pure noise\n",
    "        x = torch.randn(batch_size, 3, image_size, image_size, device=device)\n",
    "        \n",
    "        # Denoise progressively\n",
    "        timesteps = list(range(self.config.timesteps))[::-1]\n",
    "        if progress:\n",
    "            timesteps = tqdm(timesteps, desc=\"Sampling\")\n",
    "        \n",
    "        for t in timesteps:\n",
    "            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            x = self.p_sample(x, t_batch)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x_start):\n",
    "        \"\"\"\n",
    "        Training loss (simplified MSE loss).\n",
    "        \"\"\"\n",
    "        batch_size = x_start.shape[0]\n",
    "        device = x_start.device\n",
    "        \n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, self.config.timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(x_start)\n",
    "        \n",
    "        # Add noise to images\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        \n",
    "        # Predict noise\n",
    "        pred_noise = self.model(x_noisy, t)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "print(\"Diffusion process defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ema"
   },
   "source": [
    "## 5. EMA (Exponential Moving Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ema_class"
   },
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    \"\"\"\n",
    "    Exponential Moving Average of model parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.original = {}\n",
    "        \n",
    "        # Initialize shadow parameters\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self, model):\n",
    "        \"\"\"Update EMA parameters.\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] -= (1 - self.decay) * (self.shadow[name] - param.data)\n",
    "\n",
    "    def apply_shadow(self, model):\n",
    "        \"\"\"Apply EMA parameters to model.\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.original[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self, model):\n",
    "        \"\"\"Restore original parameters.\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.original[name]\n",
    "        self.original = {}\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'shadow': self.shadow, 'decay': self.decay}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.shadow = state_dict['shadow']\n",
    "        self.decay = state_dict['decay']\n",
    "\n",
    "print(\"EMA class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## 6. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_loader"
   },
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Scale to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_dataset = CIFAR10(\n",
    "    root=config.data_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.training.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.training.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset: CIFAR-10\")\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Batch size: {config.training.batch_size}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_init"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = UNet(config.model).to(device)\n",
    "diffusion = GaussianDiffusion(model, config.diffusion).to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "\n",
    "# EMA\n",
    "ema = EMA(model, decay=config.training.ema_decay)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.training.learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.0\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "def lr_lambda(step):\n",
    "    if step < config.training.warmup_steps:\n",
    "        return step / config.training.warmup_steps\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if config.training.mixed_precision else None\n",
    "\n",
    "print(\"Model initialized and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualization_utils"
   },
   "outputs": [],
   "source": [
    "# Visualization utilities\n",
    "def unnormalize(x):\n",
    "    \"\"\"Unnormalize images from [-1, 1] to [0, 1].\"\"\"\n",
    "    return (x + 1) / 2\n",
    "\n",
    "def plot_samples(samples, title=\"Generated Samples\"):\n",
    "    \"\"\"Plot a grid of samples.\"\"\"\n",
    "    samples = unnormalize(samples)\n",
    "    samples = torch.clamp(samples, 0, 1)\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(samples, nrow=8, padding=2)\n",
    "    grid_np = grid.cpu().permute(1, 2, 0).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(grid_np)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_checkpoint(step, model, optimizer, ema, scheduler, metrics, path):\n",
    "    \"\"\"Save training checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'ema': ema.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'metrics': metrics,\n",
    "        'config': config,\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved: {path}\")\n",
    "\n",
    "print(\"Utilities ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total steps: {config.training.total_steps:,}\")\n",
    "print(f\"Estimated time: ~10-11 hours on T4 GPU\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Setup directories\n",
    "checkpoint_dir = Path(config.output_dir) / \"checkpoints\"\n",
    "sample_dir = Path(config.output_dir) / \"samples\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training state\n",
    "step = 0\n",
    "running_loss = 0.0\n",
    "losses = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=config.training.total_steps, desc=\"Training\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "epoch = 0\n",
    "\n",
    "while step < config.training.total_steps:\n",
    "    epoch += 1\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        if step >= config.training.total_steps:\n",
    "            break\n",
    "        \n",
    "        images = batch[0].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                loss = diffusion(images)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss = diffusion(images)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        ema.update(model)\n",
    "        \n",
    "        step += 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Logging\n",
    "        if step % config.training.log_every == 0:\n",
    "            avg_loss = running_loss / config.training.log_every\n",
    "            losses.append(avg_loss)\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            elapsed = (datetime.now() - start_time).total_seconds()\n",
    "            steps_per_sec = step / elapsed\n",
    "            remaining_steps = config.training.total_steps - step\n",
    "            eta_seconds = remaining_steps / steps_per_sec\n",
    "            eta_hours = eta_seconds / 3600\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'lr': f'{lr:.6f}',\n",
    "                'ETA': f'{eta_hours:.1f}h'\n",
    "            })\n",
    "            \n",
    "            running_loss = 0.0\n",
    "        \n",
    "        # Generate samples\n",
    "        if step % config.training.sample_every == 0 or step == config.training.total_steps:\n",
    "            print(f\"\\n[Step {step}/{config.training.total_steps}] Generating samples...\")\n",
    "            \n",
    "            # Use EMA model for sampling\n",
    "            ema.apply_shadow(model)\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                samples = diffusion.sample(\n",
    "                    batch_size=64,\n",
    "                    image_size=config.model.image_size,\n",
    "                    progress=False\n",
    "                )\n",
    "            \n",
    "            # Save samples\n",
    "            samples_unnorm = unnormalize(samples)\n",
    "            samples_unnorm = torch.clamp(samples_unnorm, 0, 1)\n",
    "            grid = torchvision.utils.make_grid(samples_unnorm, nrow=8, padding=2)\n",
    "            save_path = sample_dir / f\"samples_step_{step:08d}.png\"\n",
    "            torchvision.utils.save_image(grid, save_path)\n",
    "            \n",
    "            # Display samples\n",
    "            clear_output(wait=True)\n",
    "            plot_samples(samples, title=f\"Samples at step {step}\")\n",
    "            \n",
    "            ema.restore(model)\n",
    "            model.train()\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if step % config.training.save_every == 0 or step == config.training.total_steps:\n",
    "            checkpoint_path = checkpoint_dir / f\"checkpoint_step_{step:08d}.pt\"\n",
    "            save_checkpoint(\n",
    "                step=step,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                ema=ema,\n",
    "                scheduler=scheduler,\n",
    "                metrics={'loss': loss.item()},\n",
    "                path=checkpoint_path\n",
    "            )\n",
    "            \n",
    "            # Also save as latest\n",
    "            latest_path = checkpoint_dir / \"checkpoint_latest.pt\"\n",
    "            save_checkpoint(\n",
    "                step=step,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                ema=ema,\n",
    "                scheduler=scheduler,\n",
    "                metrics={'loss': loss.item()},\n",
    "                path=latest_path\n",
    "            )\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Training complete\n",
    "total_time = (datetime.now() - start_time).total_seconds() / 3600\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Total time: {total_time:.2f} hours\")\n",
    "print(f\"Final loss: {loss.item():.4f}\")\n",
    "print(f\"Checkpoints saved to: {checkpoint_dir}\")\n",
    "print(f\"Samples saved to: {sample_dir}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## 8. Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_loss"
   },
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step (x100)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(config.output_dir) / 'training_loss.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_samples"
   },
   "outputs": [],
   "source": [
    "# Generate final samples\n",
    "print(\"Generating final samples with EMA model...\")\n",
    "\n",
    "ema.apply_shadow(model)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    final_samples = diffusion.sample(\n",
    "        batch_size=64,\n",
    "        image_size=config.model.image_size,\n",
    "        progress=True\n",
    "    )\n",
    "\n",
    "plot_samples(final_samples, title=\"Final Generated Samples\")\n",
    "\n",
    "# Save final samples\n",
    "final_samples_unnorm = unnormalize(final_samples)\n",
    "final_samples_unnorm = torch.clamp(final_samples_unnorm, 0, 1)\n",
    "grid = torchvision.utils.make_grid(final_samples_unnorm, nrow=8, padding=2)\n",
    "torchvision.utils.save_image(grid, Path(config.output_dir) / 'final_samples.png')\n",
    "\n",
    "ema.restore(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_summary"
   },
   "outputs": [],
   "source": [
    "# Save training summary\n",
    "summary = {\n",
    "    'exp_name': config.exp_name,\n",
    "    'total_steps': step,\n",
    "    'total_hours': total_time,\n",
    "    'timesteps': config.diffusion.timesteps,\n",
    "    'beta_schedule': config.diffusion.beta_schedule,\n",
    "    'batch_size': config.training.batch_size,\n",
    "    'learning_rate': config.training.learning_rate,\n",
    "    'final_loss': loss.item(),\n",
    "    'num_parameters': num_params,\n",
    "}\n",
    "\n",
    "summary_path = Path(config.output_dir) / 'training_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## 9. Inference - Generate More Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_more"
   },
   "outputs": [],
   "source": [
    "# Generate additional samples\n",
    "print(\"Generating additional samples...\")\n",
    "\n",
    "ema.apply_shadow(model)\n",
    "model.eval()\n",
    "\n",
    "num_batches = 4  # Generate 4 batches of 64 samples each\n",
    "\n",
    "for i in range(num_batches):\n",
    "    with torch.no_grad():\n",
    "        samples = diffusion.sample(\n",
    "            batch_size=64,\n",
    "            image_size=config.model.image_size,\n",
    "            progress=True\n",
    "        )\n",
    "    \n",
    "    plot_samples(samples, title=f\"Generated Samples Batch {i+1}\")\n",
    "\n",
    "ema.restore(model)\n",
    "\n",
    "print(f\"Generated {num_batches * 64} total samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "## Notes\n",
    "\n",
    "### Performance Expectations\n",
    "\n",
    "- **Training time**: 約10-11時間 (T4 GPU)\n",
    "- **Steps**: 100,000 steps\n",
    "- **Final FID**: 30-50程度を期待（フルトレーニングの800,000 stepsでFID ~3.17）\n",
    "\n",
    "### Tips\n",
    "\n",
    "1. **チェックポイント**: `/content/drive/MyDrive/DDPM_outputs/checkpoints/`に保存\n",
    "2. **サンプル画像**: `/content/drive/MyDrive/DDPM_outputs/samples/`に保存\n",
    "3. **接続が切れた場合**: 最新のcheckpoint (`checkpoint_latest.pt`) から再開可能\n",
    "\n",
    "### Resume Training\n",
    "\n",
    "接続が切れた場合、以下のコードで再開:\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load('/content/drive/MyDrive/DDPM_outputs/checkpoints/checkpoint_latest.pt')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "ema.load_state_dict(checkpoint['ema'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "step = checkpoint['step']\n",
    "```\n",
    "\n",
    "### References\n",
    "\n",
    "- Paper: [Denoising Diffusion Probabilistic Models (Ho et al., 2020)](https://arxiv.org/abs/2006.11239)\n",
    "- Original results: FID 3.17 on CIFAR-10 (800,000 steps)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
